{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8fd7906",
   "metadata": {},
   "source": [
    "# Certification Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041735b5",
   "metadata": {},
   "source": [
    "#### Dependencies and API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b050884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API key:\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Tavily API key:\")\n",
    "os.environ[\"YOUTUBE_API_KEY\"] = getpass.getpass(\"Youtube API key:\")\n",
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5be8bd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langsmith\n",
    "from uuid import uuid4\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING_V2\"] =\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = F\"AIE8 Certification Challenge - {uuid4().hex[0:8]}\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"Langsmith API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "bc82c81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "d1c0754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install youtube-transcript-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46036c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-cohere\n",
      "  Downloading langchain_cohere-0.4.6-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting cohere<6.0,>=5.18.0 (from langchain-cohere)\n",
      "  Downloading cohere-5.19.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-cohere) (0.3.30)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.76 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-cohere) (0.3.76)\n",
      "Requirement already satisfied: pydantic<3,>=2 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-cohere) (2.11.9)\n",
      "Collecting types-pyyaml<7.0.0.0,>=6.0.12.20240917 (from langchain-cohere)\n",
      "  Downloading types_pyyaml-6.0.12.20250915-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting fastavro<2.0.0,>=1.9.4 (from cohere<6.0,>=5.18.0->langchain-cohere)\n",
      "  Downloading fastavro-1.12.1-cp313-cp313-win_amd64.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: httpx>=0.21.2 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cohere<6.0,>=5.18.0->langchain-cohere) (0.28.1)\n",
      "Collecting httpx-sse==0.4.0 (from cohere<6.0,>=5.18.0->langchain-cohere)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cohere<6.0,>=5.18.0->langchain-cohere) (2.33.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cohere<6.0,>=5.18.0->langchain-cohere) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cohere<6.0,>=5.18.0->langchain-cohere) (0.22.1)\n",
      "Collecting types-requests<3.0.0,>=2.0.0 (from cohere<6.0,>=5.18.0->langchain-cohere)\n",
      "  Downloading types_requests-2.32.4.20250913-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0.0 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cohere<6.0,>=5.18.0->langchain-cohere) (4.14.1)\n",
      "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (0.3.27)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (2.0.43)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (3.12.14)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (2.10.1)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (0.4.6)\n",
      "Requirement already satisfied: numpy>=2.1.0 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-cohere) (2.3.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain<2.0.0,>=0.3.27->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (0.3.9)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.76->langchain-cohere) (1.33)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.76->langchain-cohere) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.76->langchain-cohere) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (3.11.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx>=0.21.2->cohere<6.0,>=5.18.0->langchain-cohere) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx>=0.21.2->cohere<6.0,>=5.18.0->langchain-cohere) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx>=0.21.2->cohere<6.0,>=5.18.0->langchain-cohere) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx>=0.21.2->cohere<6.0,>=5.18.0->langchain-cohere) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx>=0.21.2->cohere<6.0,>=5.18.0->langchain-cohere) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=2->langchain-cohere) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=2->langchain-cohere) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (1.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.0.0->cohere<6.0,>=5.18.0->langchain-cohere) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.0.0->cohere<6.0,>=5.18.0->langchain-cohere) (2.5.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (3.2.3)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tokenizers<1,>=0.15->cohere<6.0,>=5.18.0->langchain-cohere) (0.35.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.18.0->langchain-cohere) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.18.0->langchain-cohere) (2025.9.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.18.0->langchain-cohere) (4.67.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community<0.4.0,>=0.3.0->langchain-cohere) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<2.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.18.0->langchain-cohere) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio->httpx>=0.21.2->cohere<6.0,>=5.18.0->langchain-cohere) (1.3.1)\n",
      "Downloading langchain_cohere-0.4.6-py3-none-any.whl (42 kB)\n",
      "Downloading cohere-5.19.0-py3-none-any.whl (302 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading fastavro-1.12.1-cp313-cp313-win_amd64.whl (444 kB)\n",
      "Downloading types_pyyaml-6.0.12.20250915-py3-none-any.whl (20 kB)\n",
      "Downloading types_requests-2.32.4.20250913-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: types-requests, types-pyyaml, httpx-sse, fastavro, cohere, langchain-cohere\n",
      "\n",
      "  Attempting uninstall: httpx-sse\n",
      "\n",
      "    Found existing installation: httpx-sse 0.4.1\n",
      "\n",
      "    Uninstalling httpx-sse-0.4.1:\n",
      "\n",
      "      Successfully uninstalled httpx-sse-0.4.1\n",
      "\n",
      "   ------------- -------------------------- 2/6 [httpx-sse]\n",
      "   -------------------- ------------------- 3/6 [fastavro]\n",
      "   -------------------- ------------------- 3/6 [fastavro]\n",
      "   -------------------- ------------------- 3/6 [fastavro]\n",
      "   -------------------- ------------------- 3/6 [fastavro]\n",
      "   -------------------- ------------------- 3/6 [fastavro]\n",
      "   -------------------- ------------------- 3/6 [fastavro]\n",
      "   -------------------- ------------------- 3/6 [fastavro]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   -------------------------- ------------- 4/6 [cohere]\n",
      "   --------------------------------- ------ 5/6 [langchain-cohere]\n",
      "   --------------------------------- ------ 5/6 [langchain-cohere]\n",
      "   ---------------------------------------- 6/6 [langchain-cohere]\n",
      "\n",
      "Successfully installed cohere-5.19.0 fastavro-1.12.1 httpx-sse-0.4.0 langchain-cohere-0.4.6 types-pyyaml-6.0.12.20250915 types-requests-2.32.4.20250913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~rllib3 (C:\\Users\\Inés\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~rllib3 (C:\\Users\\Inés\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~rllib3 (C:\\Users\\Inés\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install langchain-cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "2d59d925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rank_bm25\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rank_bm25) (2.3.4)\n",
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: rank_bm25\n",
      "Successfully installed rank_bm25-0.2.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~rllib3 (C:\\Users\\Inés\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~rllib3 (C:\\Users\\Inés\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~rllib3 (C:\\Users\\Inés\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bc281ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-tavily\n",
      "  Downloading langchain_tavily-0.2.12-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.11.14 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-tavily) (3.12.14)\n",
      "Requirement already satisfied: langchain<2.0.0,>=0.3.20 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-tavily) (0.3.27)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=0.3.15 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-tavily) (0.3.76)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.3 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-tavily) (2.32.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (1.20.1)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain<2.0.0,>=0.3.20->langchain-tavily) (0.3.9)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain<2.0.0,>=0.3.20->langchain-tavily) (0.4.6)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain<2.0.0,>=0.3.20->langchain-tavily) (2.11.9)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain<2.0.0,>=0.3.20->langchain-tavily) (2.0.43)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain<2.0.0,>=0.3.20->langchain-tavily) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<2.0.0,>=0.3.15->langchain-tavily) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<2.0.0,>=0.3.15->langchain-tavily) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<2.0.0,>=0.3.15->langchain-tavily) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<2.0.0,>=0.3.15->langchain-tavily) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.15->langchain-tavily) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.20->langchain-tavily) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.20->langchain-tavily) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.20->langchain-tavily) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.32.3->langchain-tavily) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.32.3->langchain-tavily) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.32.3->langchain-tavily) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.32.3->langchain-tavily) (2025.6.15)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain<2.0.0,>=0.3.20->langchain-tavily) (3.2.3)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith>=0.1.17->langchain<2.0.0,>=0.3.20->langchain-tavily) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith>=0.1.17->langchain<2.0.0,>=0.3.20->langchain-tavily) (3.11.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith>=0.1.17->langchain<2.0.0,>=0.3.20->langchain-tavily) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith>=0.1.17->langchain<2.0.0,>=0.3.20->langchain-tavily) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<2.0.0,>=0.3.20->langchain-tavily) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<2.0.0,>=0.3.20->langchain-tavily) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<2.0.0,>=0.3.20->langchain-tavily) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\inés\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<2.0.0,>=0.3.20->langchain-tavily) (1.3.1)\n",
      "Downloading langchain_tavily-0.2.12-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: langchain-tavily\n",
      "Successfully installed langchain-tavily-0.2.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~rllib3 (C:\\Users\\Inés\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~rllib3 (C:\\Users\\Inés\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~rllib3 (C:\\Users\\Inés\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\Inés\\AppData\\Local\\Programs\\Python\\Python313\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install langchain-tavily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a07cd1",
   "metadata": {},
   "source": [
    "#### Initial Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9e8e0c",
   "metadata": {},
   "source": [
    "#### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27342094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4c5a9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id= 'iqNzfK4_meQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6071f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get comments\n",
    "def get_youtube_comments(video_id: str, max_comments: int = 50):\n",
    "    url = \"https://www.googleapis.com/youtube/v3/commentThreads\"\n",
    "    params = {\n",
    "        'part': 'snippet',\n",
    "        'videoId': video_id,\n",
    "        'maxResults': max_comments,\n",
    "        'key': os.environ['YOUTUBE_API_KEY']\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c12571dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get video details\n",
    "def get_video_details(video_id: str):\n",
    "    \"\"\"Get video metadata including title, description, channel info, etc.\"\"\"\n",
    "    url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
    "    params = {\n",
    "        'part': 'snippet,statistics,contentDetails',\n",
    "        'id': video_id,\n",
    "        'key': os.environ['YOUTUBE_API_KEY']\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    return response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a16a4468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get video transcrips\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "def get_video_transcript(video_id: str):\n",
    "    \"\"\"Get video transcript using YouTube Transcript API.\"\"\"\n",
    "    try:\n",
    "        api = YouTubeTranscriptApi()\n",
    "        full_transcript = api.fetch(video_id)\n",
    "        transcript_text = \" \".join(\n",
    "        full_transcript.snippets[i].text for i in range(len(full_transcript.snippets))\n",
    "            )\n",
    "        print(f\"DEBUG: Transcript fetched for {video_id}, length={len(transcript_text)}\")\n",
    "        return {\"transcript\": transcript_text}\n",
    "    \n",
    "    except Exception as exc:  # noqa: BLE001\n",
    "        print(f\"WARNING: Transcript fetch failed for {video_id}: {exc}\")\n",
    "        return {\n",
    "                \"transcript\": \"\",\n",
    "                \"error\": f\"Unable to fetch transcript: {exc}\",\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d14fc4",
   "metadata": {},
   "source": [
    "#### Create the text document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d009b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create unified document tool from video\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def create_unified_video_document(video_id: str, max_comments: int = 50):\n",
    "    \"\"\"\n",
    "    Create a unified document containing video details, transcript, and comments.\n",
    "    This gives the chatbot complete context about the video.\n",
    "    \"\"\"\n",
    "    # Get all data\n",
    "    video_details = get_video_details(video_id)\n",
    "    comments_data = get_youtube_comments(video_id, max_comments)\n",
    "    transcript_data = get_video_transcript(video_id)\n",
    "    \n",
    "    # Extract key information\n",
    "    video_info = video_details['items'][0] if video_details.get('items') else {}\n",
    "    snippet = video_info.get('snippet', {})\n",
    "    statistics = video_info.get('statistics', {})\n",
    "    \n",
    "    # Format comments\n",
    "    formatted_comments = []\n",
    "    if 'items' in comments_data:\n",
    "        for item in comments_data['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']\n",
    "            formatted_comments.append({\n",
    "                'text': comment.get('textDisplay', ''),\n",
    "                'author': comment.get('authorDisplayName', ''),\n",
    "                'likes': comment.get('likeCount', 0),\n",
    "                'published': comment.get('publishedAt', '')\n",
    "            })\n",
    "    \n",
    "    # Create unified content\n",
    "    unified_content = f\"\"\"\n",
    "# VIDEO ANALYSIS CONTEXT\n",
    "\n",
    "## Video Information\n",
    "**Title:** {snippet.get('title', 'N/A')}\n",
    "**Channel:** {snippet.get('channelTitle', 'N/A')}\n",
    "**Published:** {snippet.get('publishedAt', 'N/A')}\n",
    "**Views:** {statistics.get('viewCount', 'N/A')}\n",
    "**Likes:** {statistics.get('likeCount', 'N/A')}\n",
    "**Comments Count:** {statistics.get('commentCount', 'N/A')}\n",
    "\n",
    "## Video Description\n",
    "{snippet.get('description', 'No description available')}\n",
    "\n",
    "## Video Transcript\n",
    "{transcript_data.get('transcript', 'No transcription available')}\n",
    "\n",
    "## Comments Analysis\n",
    "**Total Comments Analyzed:** {len(formatted_comments)}\n",
    "\n",
    "### Comment Details:\n",
    "\"\"\"\n",
    "    \n",
    "    # Add individual comments\n",
    "    for i, comment in enumerate(formatted_comments, 1):\n",
    "        unified_content += f\"\"\"\n",
    "**Comment {i}:**\n",
    "- Author: {comment['author']}\n",
    "- Likes: {comment['likes']}\n",
    "- Published: {comment['published']}\n",
    "- Text: {comment['text']}\n",
    "---\n",
    "\"\"\"\n",
    "    \n",
    "    # Create the document\n",
    "    unified_document = Document(\n",
    "        page_content=unified_content,\n",
    "        metadata={\n",
    "            \"type\": \"unified_video_analysis\",\n",
    "            \"video_id\": video_id,\n",
    "            \"title\": snippet.get('title', ''),\n",
    "            \"channel\": snippet.get('channelTitle', ''),\n",
    "            \"comment_count\": len(formatted_comments),\n",
    "            \"has_transcript\": bool(transcript_data.get('transcript')),\n",
    "            \"views\": statistics.get('viewCount', 0),\n",
    "            \"likes\": statistics.get('likeCount', 0),\n",
    "            \"published\": snippet.get('publishedAt', ''),\n",
    "            \"source\": \"youtube_unified\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return unified_document, {\n",
    "        'video_details': video_details,\n",
    "        'comments': comments_data,\n",
    "        'transcript': transcript_data,\n",
    "        'formatted_comments': formatted_comments\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86517105",
   "metadata": {},
   "source": [
    "#### Organize document for vector database, context + comments, with metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792b3d39",
   "metadata": {},
   "source": [
    ">Note To illustrate chunking we will used it, in case there is a transcript and it is long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b77b2190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Transcript fetched for iqNzfK4_meQ, length=5854\n"
     ]
    }
   ],
   "source": [
    "#1. build structured Document objects\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "unified_document, raw_blobs = create_unified_video_document(video_id)\n",
    "formatted_comments = raw_blobs[\"formatted_comments\"]\n",
    "\n",
    "# video-level context document\n",
    "context_doc = Document(\n",
    "    page_content=unified_document.page_content,\n",
    "    metadata={\n",
    "        \"type\": \"video_context\",\n",
    "        \"video_id\": unified_document.metadata[\"video_id\"],\n",
    "        \"title\": unified_document.metadata.get(\"title\", \"\"),\n",
    "        \"channel\": unified_document.metadata.get(\"channel\", \"\"),\n",
    "        \"source\": \"youtube_unified\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# one document per comment with rich payload\n",
    "comment_docs = [\n",
    "    Document(\n",
    "        page_content=comment[\"text\"],\n",
    "        metadata={\n",
    "            \"type\": \"comment\",\n",
    "            \"comment_index\": idx + 1,\n",
    "            \"author\": comment[\"author\"],\n",
    "            \"likes\": comment[\"likes\"],\n",
    "            \"published\": comment[\"published\"],\n",
    "            \"video_id\": unified_document.metadata[\"video_id\"],\n",
    "            \"title\": unified_document.metadata.get(\"title\", \"\"),\n",
    "        },\n",
    "    )\n",
    "    for idx, comment in enumerate(formatted_comments)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d38ea85d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'type': 'unified_video_analysis', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'comment_count': 50, 'has_transcript': True, 'views': '134177', 'likes': '3042', 'published': '2025-10-14T18:19:27Z', 'source': 'youtube_unified'}, page_content='\\n# VIDEO ANALYSIS CONTEXT\\n\\n## Video Information\\n**Title:** Using OpenAI Codex CLI with GPT-5-Codex\\n**Channel:** OpenAI\\n**Published:** 2025-10-14T18:19:27Z\\n**Views:** 134177\\n**Likes:** 3042\\n**Comments Count:** 265\\n\\n## Video Description\\nLearn how to use the Codex CLI + GPT-5-Codex in just 5 minutes.\\n\\n\\nIn this tutorial, Eason Goodale and Romain Huet show you how to build a multiplayer game with Codex — without writing a single line of code! \\n\\nTimestamps:\\n\\n00:00 Intro\\n00:50 Planning the multiplayer implementation\\n1:38 Using CLI commands\\n2:55 Implementing the plan\\n3:31 Deploy the game\\n4:20 Playing the game\\n\\n\\nTo learn more and get started:\\n\\nCodex: openai.com/codex \\nCodex CLI GitHub repo: https://github.com/openai/codex \\nDeveloper docs: developers.openai.com/codex\\n\\n## Video Transcript\\nHey, what are you working on? >> I\\'m trying to find a good demo. Something that Codex can modify. We could make this little ball thing multiplayer. >> That sounds very cool. Let\\'s do it. >> Codeex CL164. Mark. >> Hey everyone, Roma here. Recently, we ship GP5 and GP5 codecs and we\\'ve also released a ton of improvements to Codex CLI to better harness the agentic coding capabilities of these models. And today I\\'m sitting with Eson who led a lot of this effort on the CLI. Do you want to give us a quick tour? >> Yeah, I\\'d love to. Um, we have tons of really cool updates. You can install it really easily with either mpm or brew and log in with your chat GPT account. >> So here you\\'re in your terminal and you just have to launch it with by codex. >> That\\'s all there is to it. So we\\'ll say make a plan for making this game multiplayer. What\\'s funny is like this game was one of the very many examples we shipped completely built by GPT5 in one prompt. >> Yes. >> Like and now we can start building up upon it. So what it\\'s thinking tell us a little more like about what\\'s happening which model you\\'re using here. >> Yeah. So this is uh going to be GPT5 codeex which is our new model and it\\'s really good for any sort of coding task. >> So here it\\'s like currently crafting the plan. I see it\\'s laying out the steps of what it\\'s supposed to do. Yep. >> Can you expand what\\'s happening here? Yeah, totally. So, we can go into transcript mode with control T and that gives you things that are super useful like the uh the chain of thought, the sort of the exact code that it\\'s doing. >> If you don\\'t not interested into the the whole details, you can just like let it uh run at a very high level telling you like what it\\'s doing. >> Yes, exactly. >> Okay. So, while it\\'s working on this like kind of multiplayer feature, why don\\'t you kind of open up a second codeex to give us maybe a quick tour of some of your favorite commands? >> Yeah, so I\\'m a huge fan of the model switcher. You sometimes want to use one model for one thing, one model for another. This allows you to change the reasoning level, >> right? Because with the new GP5 codeex model, the very simple task can go very fast. >> Yes. >> But for the more advanced one, now Codex can work on for like up to hours at a time. Okay. So that\\'s SL model. What else? >> Yeah. Uh approvals is really useful. So this is where you kind of get into the sandboxing features of codecs which are very cool, very powerful. We have three modes. We have read only, we have auto, and we have full access. Auto is the default. And that allows codeex to read files and make changes to files within the current directory. So by default, it stays in the boundaries of your project. It\\'s not going to affect anything else on your laptop. >> Exactly. And then if you want to be in readon, that\\'s kind of useful for, for example, running outside of a git repository. Y or if you\\'re like, I only care about planning. I actually don\\'t want Codex get distracted by trying to edit things. And then we have Codex resume. And that allows you to pick up from any previous session. Super nice. >> Why don\\'t you uh go check back on the status of this multiplayer game? >> Yeah, it looks like we\\'ve got a plan. So why don\\'t we tell Codeex to do that? >> Great. >> So one of the things that I think is really interesting that people sort of miss about Codeex is that it\\'s useful for these coding tasks, but you can also deploy things with it. You can use it for S sur type things. You can figure out like, oh, we\\'re seeing these, you know, this bug show up for our users. Why is this showing up? Go look at the logs. Um, take these disparate data sources, combine them. Um, it\\'s surprisingly very, very, very good at that sort of thing. How are we doing on the game? >> Yeah, I think the game is probably good to go. >> So, it sounds like the the moment of truth is to play the game, but maybe before we need to deploy it. >> Yeah. So what I\\'m going to say for this app, let\\'s maybe uh deploy it on Versal. >> Yep. >> And let\\'s use code- search in order to tell it to hey look up the latest versal docs. >> Yeah. In case like you want to deploy something very specific and you need persistence or maybe you want to look up the latest changes of like an API. >> Yeah, exactly. We should go to approval. We should switch it to full access. And then we\\'ll tell it use the Versel command line tool to deploy this app. >> Cool. Sounds like it\\'s deployed. >> Yeah, let\\'s do it. >> Should we try it? >> Yeah, let\\'s give it a shot. >> So, I guess I can take over this laptop. If you want to bring yours, I\\'m going to have to ping you the link. >> Yep. >> There you go. You should have it. >> Ready to start. >> Let\\'s go. Oh my god, this is awesome. We are really in sync. >> Yeah, super in sync. >> Incredible. >> This is all real time. >> Who\\'s going to be the best at this? I don\\'t know. >> You seem pretty good. >> Ah, okay. >> To wrap us up, what have we seen? So, we saw Codex CLI logged into your chat GPD subscription >> starting to like change a game. >> Yep. >> Make a plan to implement like a full multiplayer game. >> Yep. We saw a quick tour of the commands, but more interestingly, you use web search to kind of like fetch information from the internet. You change approval modes. We deployed this game and we\\'re now able to to play it. >> Yeah, it\\'s super easy. This is the exact same flow that I use to, you know, do pretty serious stuff just across like a wide variety of languages, a wide variety of frameworks, wide variety of projects. Amazing. Well, as you can tell, we\\'re shipping a ton of improvements across all codec surfaces. So you can have this AI teammate at your disposal wherever you work and in this case right in the terminal. And we can\\'t wait to see what you build with Codex CLI. See you next time.\\n\\n## Comments Analysis\\n**Total Comments Analyzed:** 50\\n\\n### Comment Details:\\n\\n**Comment 1:**\\n- Author: @FukdUohp\\n- Likes: 0\\n- Published: 2025-10-17T18:17:58Z\\n- Text: Very nicely done.\\n---\\n\\n**Comment 2:**\\n- Author: @zerosiii\\n- Likes: 0\\n- Published: 2025-10-17T17:56:48Z\\n- Text: This is really great, but some feedback for OpenAI (or anyone else building these sorts of things): Agents are just one part of the puzzle, the second part is deployment. This really needs to be made as hassle free and user friendly as possible for the majority of people. A popular stack is Vercel and Supabase, but Replit for example handles everything on their end - you use their agent (as you would codex), and the agent instantiates the Neon database, deploys on GCP, and Replit handles this infrastructure for you. <br>I would LOVE to see more companies (OpenAI especially) offer this kind of end-to-end services, as it would allow us (the users) to build our apps entirely within your ecosystem.\\n---\\n\\n**Comment 3:**\\n- Author: @BatAttackZero\\n- Likes: 0\\n- Published: 2025-10-17T17:24:54Z\\n- Text: Dude looks like a novice necromancer.\\n---\\n\\n**Comment 4:**\\n- Author: @jessegador\\n- Likes: 0\\n- Published: 2025-10-17T16:10:56Z\\n- Text: Waiting on the folder exclusion feature :)\\n---\\n\\n**Comment 5:**\\n- Author: @Crazy_Cat-2025\\n- Likes: 0\\n- Published: 2025-10-17T10:34:42Z\\n- Text: Can users use the previous ChatGPT-4.5?\\n---\\n\\n**Comment 6:**\\n- Author: @kurrubmapu6690\\n- Likes: 0\\n- Published: 2025-10-16T19:12:44Z\\n- Text: El modelo de negocio actual de las IAs es muy oscuro en el sentido de que pagas por algo que no sabes cuánto te va a durar.<br>En la versión free a los 5 minutos te dice :  Acabas de alcanzar tu límite, Actualiza a Pro etc.. Luego vas a pro y te preguntas ¿Cuánto me alcanza con esta sub? Y te dicen &quot;mas uso que free&quot; Todo entre tinieblas. ¿Es legal esto? <br>Si tu compras 1 kg de carbón sabes que te alcanzará para asar una carne.\\n---\\n\\n**Comment 7:**\\n- Author: @Leotique\\n- Likes: 0\\n- Published: 2025-10-16T19:09:50Z\\n- Text: great, I build games with each kid at school\\n---\\n\\n**Comment 8:**\\n- Author: @claudioagmfilho\\n- Likes: 1\\n- Published: 2025-10-16T18:40:03Z\\n- Text: 🇧🇷🇧🇷👏🏻\\n---\\n\\n**Comment 9:**\\n- Author: @PeterAdiSaputro\\n- Likes: 0\\n- Published: 2025-10-16T18:17:02Z\\n- Text: Can I used with my free account ?\\n---\\n\\n**Comment 10:**\\n- Author: @martin-braun\\n- Likes: 0\\n- Published: 2025-10-16T17:42:48Z\\n- Text: The best part about Codex is your code will have more holes than Swiss Cheese, so the getting-hacked part comes right with it.\\n---\\n\\n**Comment 11:**\\n- Author: @JohnSmith762A11B\\n- Likes: 0\\n- Published: 2025-10-16T16:08:41Z\\n- Text: GPT-5-Codex is the most fun a developer can have creating software. It&#39;s less like work and more like... conjuring new apps and features.\\n---\\n\\n**Comment 12:**\\n- Author: @Daniel-y5m1l\\n- Likes: 0\\n- Published: 2025-10-16T15:19:18Z\\n- Text: <a href=\"https://www.youtube.com/watch?v=iqNzfK4_meQ&amp;t=243\">4:03</a> i&#39;d be too scared to give codex &quot;full access&quot;. isn&#39;t there a non-zero danger that it does something with the vercel api/mcp that costs me a lot of money? is there a way to exercise more fine-grained control over what codex is allowed to do (and how often) and what not?\\n---\\n\\n**Comment 13:**\\n- Author: @steinbeats\\n- Likes: 0\\n- Published: 2025-10-16T11:32:50Z\\n- Text: Terminal on light mode hmm\\n---\\n\\n**Comment 14:**\\n- Author: @alexasiricortanayuen7847\\n- Likes: 0\\n- Published: 2025-10-16T10:55:34Z\\n- Text: I’ve been using Codex for almost a month now, and the results have been great. At some point, it seemed to know the project better than I did. LOLOL. It even argued against my instruction and I argued back irritably until I realized that the responses made sense. Codex was right, and I was wrong. My initial prompt referred to Y instead of X. I assumed we were talking about X, and it kept insisting it couldn’t be done on X, while I kept pushing for what I wanted. I like that it doesn’t always agree with me haha and do whatever I say. To me It&#39;s more helpful that way.\\n---\\n\\n**Comment 15:**\\n- Author: @SamiritanPhilanth81\\n- Likes: 0\\n- Published: 2025-10-16T10:41:13Z\\n- Text: Please release new update patches for chatgpt on android!\\n---\\n\\n**Comment 16:**\\n- Author: @0x92dev5\\n- Likes: 0\\n- Published: 2025-10-16T08:33:45Z\\n- Text: I still prefer Codex Cloud IDE, no limits, better overview of diffs\\n---\\n\\n**Comment 17:**\\n- Author: @DrHumorous\\n- Likes: 0\\n- Published: 2025-10-16T08:30:32Z\\n- Text: Codex is my best friend lately\\n---\\n\\n**Comment 18:**\\n- Author: @gino______2485\\n- Likes: 0\\n- Published: 2025-10-16T08:12:21Z\\n- Text: It&#39;s a powerful tool, I have been using it too❤\\n---\\n\\n**Comment 19:**\\n- Author: @endoflevelboss\\n- Likes: 0\\n- Published: 2025-10-16T08:10:49Z\\n- Text: How did that guy&#39;s haircut be allowed on camera?\\n---\\n\\n**Comment 20:**\\n- Author: @stym06\\n- Likes: 1\\n- Published: 2025-10-16T04:58:35Z\\n- Text: Adam Driver is great in this video\\n---\\n\\n**Comment 21:**\\n- Author: @outsidefive\\n- Likes: 0\\n- Published: 2025-10-16T03:56:19Z\\n- Text: Hey why is it not working? I don&#39;t know, ask GPT. Rip Software Engineering.\\n---\\n\\n**Comment 22:**\\n- Author: @AnonDev77\\n- Likes: 0\\n- Published: 2025-10-16T02:17:56Z\\n- Text: get Eson(?) on more of these videos.  thumbs up. loving codex\\n---\\n\\n**Comment 23:**\\n- Author: @scatterbrain-r5j\\n- Likes: 0\\n- Published: 2025-10-16T02:13:38Z\\n- Text: It would have been hilarious if he made the game too easy but they were both way too competitive so the gaming sequence is just them sitting in silence for 4 hours playing this ball game trying not to lose.\\n---\\n\\n**Comment 24:**\\n- Author: @snapperAI\\n- Likes: 0\\n- Published: 2025-10-16T01:42:04Z\\n- Text: Codex CLI is great. I&#39;ve created a CLI Mastery deep dive series on my channel, custom slash commands is the game changer for me.\\n---\\n\\n**Comment 25:**\\n- Author: @jamaalcodes\\n- Likes: 0\\n- Published: 2025-10-16T01:15:03Z\\n- Text: I need this long haired guys github lol\\n---\\n\\n**Comment 26:**\\n- Author: @remlik\\n- Likes: 0\\n- Published: 2025-10-16T00:34:17Z\\n- Text: what kind of psycho uses terminal in light<br>mode\\n---\\n\\n**Comment 27:**\\n- Author: @dandragomir6413\\n- Likes: 0\\n- Published: 2025-10-16T00:23:30Z\\n- Text: are all comments here written by bots? thats what it seems like\\n---\\n\\n**Comment 28:**\\n- Author: @attribute-4677\\n- Likes: 0\\n- Published: 2025-10-15T23:43:14Z\\n- Text: 🥱 same ole slop\\n---\\n\\n**Comment 29:**\\n- Author: @horus4862\\n- Likes: 0\\n- Published: 2025-10-15T20:59:49Z\\n- Text: crap!\\n---\\n\\n**Comment 30:**\\n- Author: @nicko_ai1\\n- Likes: 0\\n- Published: 2025-10-15T20:20:56Z\\n- Text: The game is still deployed, I played a little bit 😅\\n---\\n\\n**Comment 31:**\\n- Author: @SriramSridhar11\\n- Likes: 0\\n- Published: 2025-10-15T19:20:00Z\\n- Text: <a href=\"https://www.youtube.com/watch?v=iqNzfK4_meQ&amp;t=286\">4:46</a> who is the winner? can you change the code and make it show the winner? (:\\n---\\n\\n**Comment 32:**\\n- Author: @user-13853jxjdd\\n- Likes: 0\\n- Published: 2025-10-15T18:24:06Z\\n- Text: Honestly, I was using it like crazy two months ago, but the rate limit ran out so fast… so fast. Not only that, but it also had a really long cooldown. Has my long-haired brother fixed this for me?\\n---\\n\\n**Comment 33:**\\n- Author: @kurogiri-h5l\\n- Likes: 0\\n- Published: 2025-10-15T17:25:27Z\\n- Text: model terbaru yakni gpt 5, masih memiliki banyak kekurangan, seperti adanya fitur &#39;berpikir lebih panjang untuk jawaban lebih baik&#39; lalu delay reset free yang lama seharian. itu sangat tidak nyaman dan membuat user lebih cepat bosan dengan adanya kekurangan tersebut, contoh dari pengalaman saya. ketika saya bermain roleplay dengan chatgpt model gpt 5,saat saya membuat narasi dan dialog untuk dikirimkan ke chatgpt, selalu muncul fitur &#39;berpikir lebih panjang untuk jawaban lebih baik&#39; jika saya biarkan selalu saja responnya keluar konteks sama apa yang saya kirim. respon selalu saja tidak konsisten pada saat user mengirim perintah. tolong di perbaiki, karena sudah lama adanya fitur busuk tersebut. kalau user free terus-terusan diberi fitur, kendali, konten, dan respon yang sangat minim, maka dipastikan para user free tidak akan mau lagi untuk menggunakan chatgpt. saya harap curhatan dan kritik saya dibaca dan direspon dengan baik. saya hanya ingin diperbaiki lagi kekurangan yang saya sampaikan sebelumnya\\n---\\n\\n**Comment 34:**\\n- Author: @janosorcsik\\n- Likes: 0\\n- Published: 2025-10-15T17:03:26Z\\n- Text: Why is what the guy is saying different from what&#39;s shown in the video?\\n---\\n\\n**Comment 35:**\\n- Author: @repoles\\n- Likes: 0\\n- Published: 2025-10-15T16:57:11Z\\n- Text: My eyes hurt with this color scheme 😎\\n---\\n\\n**Comment 36:**\\n- Author: @xeoxaz\\n- Likes: 1\\n- Published: 2025-10-15T16:45:32Z\\n- Text: Is this whole video AI?\\n---\\n\\n**Comment 37:**\\n- Author: \\n- Likes: 0\\n- Published: 2025-10-15T16:28:02Z\\n- Text: Abicim, birazcık daha complex projeler, bir go servisi olur, bir django projesi olur, büyük bi projede demo yapın ya. paso uydur kaydır javascript&#39;le demo yapmayın, hayvanlama bir Java banking projesinde nasıl kullanıyorsunuz onları gösterin. hep oyuncak projeler... işi içine servisler girsin, database girsin, hardocore canlı çalışan bir şey gösterin ya...\\n---\\n\\n**Comment 38:**\\n- Author: @lee_0_jeong434\\n- Likes: 1\\n- Published: 2025-10-15T16:22:30Z\\n- Text: I belive Open AI! You&#39;ll improve more powerful Codex😊\\n---\\n\\n**Comment 39:**\\n- Author: @tuhpvn\\n- Likes: 0\\n- Published: 2025-10-15T16:21:36Z\\n- Text: codex vs code extension is suck\\n---\\n\\n**Comment 40:**\\n- Author: @mistersunday_\\n- Likes: 0\\n- Published: 2025-10-15T15:52:29Z\\n- Text: There&#39;s a lot you can infer about OpenAI culture by observing both.\\n---\\n\\n**Comment 41:**\\n- Author: @snylekkie\\n- Likes: 0\\n- Published: 2025-10-15T15:42:43Z\\n- Text: That’s fake and scripted\\n---\\n\\n**Comment 42:**\\n- Author: @KarpetLaut\\n- Likes: 0\\n- Published: 2025-10-15T15:04:55Z\\n- Text: I think i will lost my job...\\n---\\n\\n**Comment 43:**\\n- Author: @brThefox\\n- Likes: 0\\n- Published: 2025-10-15T14:43:31Z\\n- Text: Claude make better UI.<br>Can you guys make Codex to be at the same level? 🙏\\n---\\n\\n**Comment 44:**\\n- Author: @alisufyan6784\\n- Likes: 0\\n- Published: 2025-10-15T14:34:26Z\\n- Text: At least keep the VS CODE EXTENSION 😢😢😢😢😢\\n---\\n\\n**Comment 45:**\\n- Author: @barbudania\\n- Likes: 0\\n- Published: 2025-10-15T14:11:15Z\\n- Text: I&#39;m glad one of Lord Walder Frey&#39;s sons made a career in programming\\n---\\n\\n**Comment 46:**\\n- Author: @Optimusprime_1342\\n- Likes: 0\\n- Published: 2025-10-15T13:45:34Z\\n- Text: Hi OpenAI support i have been experiencing many issues on iOS/android can you fix the bug i tried send image and it logged me out please fix this bug why it logged me out for no reason\\n---\\n\\n**Comment 47:**\\n- Author: @openmictea\\n- Likes: 0\\n- Published: 2025-10-15T13:04:43Z\\n- Text: This demo is pretty misleading. Until very recently, users weren’t even told Codex was accessible inside ChatGPT, and the version shown here looks fully autonomous — running complex tasks for hours without input. After watching the video, we worked with ChatGPT to try the same autonomous coding and learned that what’s actually available still requires constant prompting and supervision. It’s not the autonomous system the video implies, and OpenAI should be clearer about what’s truly released versus what’s still in internal testing.\\n---\\n\\n**Comment 48:**\\n- Author: @shitushehuusman1289\\n- Likes: 0\\n- Published: 2025-10-15T12:38:44Z\\n- Text: Why is Codex so bad in VS Code on Windows?\\n---\\n\\n**Comment 49:**\\n- Author: @phyy2c\\n- Likes: 0\\n- Published: 2025-10-15T12:33:10Z\\n- Text: Did you guys brought random programmers in OpenAI and ordered them to take advertisements?\\n---\\n\\n**Comment 50:**\\n- Author: @TEAMZ-a\\n- Likes: 0\\n- Published: 2025-10-15T12:29:17Z\\n- Text: Maybe I&#39;m really bad for this game, but as a french, the second guy (not the dev) is French. Tell me if it&#39;s easier to notice as an English speaker.\\n---\\n')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unified_document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f64bf6",
   "metadata": {},
   "source": [
    "#### Simple LangGraph RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf9ba7d",
   "metadata": {},
   "source": [
    "#### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48b5ea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R (Naive Retrieval)\n",
    "\n",
    "# 2. Chunk only the long context (long transcripts)\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=750,\n",
    "    chunk_overlap=150,\n",
    ")\n",
    "\n",
    "context_chunks = text_splitter.split_documents([context_doc])\n",
    "docs_for_store = context_chunks + comment_docs # no chunking comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6809174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\") # Sufficient for comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcc982fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 88 document chunks to the vector store\n",
      "Vector store collection: video_sentiment_data\n",
      "Embedding model: text-embedding-3-small (1536 dimensions)\n"
     ]
    }
   ],
   "source": [
    "# QDrant vectorstore\n",
    "\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"video_sentiment_data\",\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"video_sentiment_data\",\n",
    "    embedding=embedding,\n",
    ")\n",
    "\n",
    "# Add the chunked documents to the vector store\n",
    "_ = vector_store.add_documents(docs_for_store)\n",
    "\n",
    "print(f\"Added {len(docs_for_store)} document chunks to the vector store\")\n",
    "print(f\"Vector store collection: video_sentiment_data\")\n",
    "print(f\"Embedding model: text-embedding-3-small (1536 dimensions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0e75fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_retriever = vector_store.as_retriever(\n",
    "    search_kwargs = {\"k\":6}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b3d0ce",
   "metadata": {},
   "source": [
    "#### Augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8eb57f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "HUMAN_TEMPLATE = \"\"\"\n",
    "#CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUERY:\n",
    "{query}\n",
    "\n",
    "\"Use the provided context, which consists of YouTube comments, \"\n",
    "\"to answer the user query. Only use the provided context to answer the query.\" \n",
    "\"When forming your response, take into account the topics discussed, the users who made the comments, \" \n",
    "\"and the sentiment expressed in the comments to increase factual correctness and answer relevancy. \" \n",
    "\"The chatbot is intended to answer questions about users' opinions of the video. \" \n",
    "\"If you do not know the answer, or it is not contained in the provided context, respond with \"I don't know.\"\"\n",
    "\"\"\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", HUMAN_TEMPLATE)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf297bb2",
   "metadata": {},
   "source": [
    "#### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8ae8e92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generator_llm = ChatOpenAI(model = \"gpt-4.1-nano\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc3e8cd",
   "metadata": {},
   "source": [
    "RAG - Retrieval Augmented Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de8f0cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    response: str\n",
    "\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = qdrant_retriever.invoke(state['question'])\n",
    "    return {\"context\": retrieved_docs}\n",
    "    \n",
    "\n",
    "def generate(state: State):\n",
    "    generator_chain = chat_prompt | generator_llm | StrOutputParser()\n",
    "    response = generator_chain.invoke({\"query\": state['question'], \"context\": state[\"context\"]})\n",
    "    return {'response': response}\n",
    "\n",
    "rag_graph = StateGraph(State).add_sequence([retrieve, generate])\n",
    "rag_graph.add_edge(START, \"retrieve\")\n",
    "compiled_rag_graph = rag_graph.compile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "425b1541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Which are most comments saying',\n",
       " 'context': [Document(metadata={'type': 'comment', 'comment_index': 27, 'author': '@dandragomir6413', 'likes': 0, 'published': '2025-10-16T00:23:30Z', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', '_id': '3b479fbeef424023b61a56c8fd770b9d', '_collection_name': 'video_sentiment_data'}, page_content='are all comments here written by bots? thats what it seems like'),\n",
       "  Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified', '_id': '310a06c15dbc45a686902704788e5d21', '_collection_name': 'video_sentiment_data'}, page_content='**Comment 25:**\\n- Author: @jamaalcodes\\n- Likes: 0\\n- Published: 2025-10-16T01:15:03Z\\n- Text: I need this long haired guys github lol\\n---\\n\\n**Comment 26:**\\n- Author: @remlik\\n- Likes: 0\\n- Published: 2025-10-16T00:34:17Z\\n- Text: what kind of psycho uses terminal in light<br>mode\\n---\\n\\n**Comment 27:**\\n- Author: @dandragomir6413\\n- Likes: 0\\n- Published: 2025-10-16T00:23:30Z\\n- Text: are all comments here written by bots? thats what it seems like\\n---\\n\\n**Comment 28:**\\n- Author: @attribute-4677\\n- Likes: 0\\n- Published: 2025-10-15T23:43:14Z\\n- Text: 🥱 same ole slop\\n---\\n\\n**Comment 29:**\\n- Author: @horus4862\\n- Likes: 0\\n- Published: 2025-10-15T20:59:49Z\\n- Text: crap!\\n---'),\n",
       "  Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified', '_id': 'b13ff63d71d3432c982488778f3a612b', '_collection_name': 'video_sentiment_data'}, page_content='**Comment 19:**\\n- Author: @endoflevelboss\\n- Likes: 0\\n- Published: 2025-10-16T08:10:49Z\\n- Text: How did that guy&#39;s haircut be allowed on camera?\\n---\\n\\n**Comment 20:**\\n- Author: @stym06\\n- Likes: 1\\n- Published: 2025-10-16T04:58:35Z\\n- Text: Adam Driver is great in this video\\n---\\n\\n**Comment 21:**\\n- Author: @outsidefive\\n- Likes: 0\\n- Published: 2025-10-16T03:56:19Z\\n- Text: Hey why is it not working? I don&#39;t know, ask GPT. Rip Software Engineering.\\n---\\n\\n**Comment 22:**\\n- Author: @AnonDev77\\n- Likes: 0\\n- Published: 2025-10-16T02:17:56Z\\n- Text: get Eson(?) on more of these videos.  thumbs up. loving codex\\n---'),\n",
       "  Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified', '_id': 'f5238e20a4954f6a848a1a08c4f70e59', '_collection_name': 'video_sentiment_data'}, page_content='**Comment 15:**\\n- Author: @SamiritanPhilanth81\\n- Likes: 0\\n- Published: 2025-10-16T10:41:13Z\\n- Text: Please release new update patches for chatgpt on android!\\n---\\n\\n**Comment 16:**\\n- Author: @0x92dev5\\n- Likes: 0\\n- Published: 2025-10-16T08:33:45Z\\n- Text: I still prefer Codex Cloud IDE, no limits, better overview of diffs\\n---\\n\\n**Comment 17:**\\n- Author: @DrHumorous\\n- Likes: 0\\n- Published: 2025-10-16T08:30:32Z\\n- Text: Codex is my best friend lately\\n---\\n\\n**Comment 18:**\\n- Author: @gino______2485\\n- Likes: 0\\n- Published: 2025-10-16T08:12:21Z\\n- Text: It&#39;s a powerful tool, I have been using it too❤\\n---'),\n",
       "  Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified', '_id': '5e8dcd8bd22649778a74ec059be9523a', '_collection_name': 'video_sentiment_data'}, page_content='**Comment 44:**\\n- Author: @alisufyan6784\\n- Likes: 0\\n- Published: 2025-10-15T14:34:26Z\\n- Text: At least keep the VS CODE EXTENSION 😢😢😢😢😢\\n---\\n\\n**Comment 45:**\\n- Author: @barbudania\\n- Likes: 0\\n- Published: 2025-10-15T14:11:15Z\\n- Text: I&#39;m glad one of Lord Walder Frey&#39;s sons made a career in programming\\n---\\n\\n**Comment 46:**\\n- Author: @Optimusprime_1342\\n- Likes: 0\\n- Published: 2025-10-15T13:45:34Z\\n- Text: Hi OpenAI support i have been experiencing many issues on iOS/android can you fix the bug i tried send image and it logged me out please fix this bug why it logged me out for no reason\\n---'),\n",
       "  Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified', '_id': 'fc0dfc82466f455d845f28a879d38601', '_collection_name': 'video_sentiment_data'}, page_content='**Comment 39:**\\n- Author: @tuhpvn\\n- Likes: 0\\n- Published: 2025-10-15T16:21:36Z\\n- Text: codex vs code extension is suck\\n---\\n\\n**Comment 40:**\\n- Author: @mistersunday_\\n- Likes: 0\\n- Published: 2025-10-15T15:52:29Z\\n- Text: There&#39;s a lot you can infer about OpenAI culture by observing both.\\n---\\n\\n**Comment 41:**\\n- Author: @snylekkie\\n- Likes: 0\\n- Published: 2025-10-15T15:42:43Z\\n- Text: That’s fake and scripted\\n---\\n\\n**Comment 42:**\\n- Author: @KarpetLaut\\n- Likes: 0\\n- Published: 2025-10-15T15:04:55Z\\n- Text: I think i will lost my job...\\n---\\n\\n**Comment 43:**\\n- Author: @brThefox\\n- Likes: 0\\n- Published: 2025-10-15T14:43:31Z\\n- Text: Claude make better UI.<br>Can you guys make Codex to be at the same level? 🙏\\n---')],\n",
       " 'response': 'Based on the provided comments, most users seem to have neutral to positive opinions about the video. Several comments express appreciation for the tools and technologies discussed, such as Codex and its features (\"Codex is my best friend lately,\" \"It’s a powerful tool,\" \"loving codex\"). Some users express specific preferences, like preferring the Codex Cloud IDE or requesting feature updates (e.g., the VS Code extension). A few comments are negative or critical, mentioning issues or dissatisfaction (\"crap!\", \"codex vs code extension is suck,\" \"That’s fake and scripted,\" \"I think I will lose my job...\"). There is also a comment questioning whether comments might be bots, but overall, the majority appear to have a positive or neutral tone regarding the content of the video and the discussed tools.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compiled_rag_graph.invoke({'question':'Which are most comments saying'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf46ffa5",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "128a7e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, List, Optional, TypedDict, Union\n",
    "\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74eeb6b",
   "metadata": {},
   "source": [
    "##### Helper functions to create node, agents, supervisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ee05d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent node helper\n",
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    return {\n",
    "        \"messages\": [HumanMessage(content=result[\"output\"], name=name)],\n",
    "        \"documents\": result.get(\"documents\", [])\n",
    "        \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33b71bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom node for CommentFinder as it needs to return the documents\n",
    "def agent_node_with_docs(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    output = result[\"output\"]\n",
    "    \n",
    "    # Try to extract documents if this is CommentFinder\n",
    "    documents = []\n",
    "    if name == \"CommentFinder\":\n",
    "        # Parse the output to extract documents if they're returned\n",
    "        # The tool returns a dict, but agent wraps it as string\n",
    "        # We need to access the raw tool result\n",
    "        try:\n",
    "            # Get the intermediate steps from agent execution\n",
    "            if \"intermediate_steps\" in result:\n",
    "                for action, observation in result[\"intermediate_steps\"]:\n",
    "                    if isinstance(observation, dict) and \"documents\" in observation:\n",
    "                        documents = observation[\"documents\"]\n",
    "        except:\n",
    "            documents = []\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [HumanMessage(content=output, name=name)],\n",
    "        \"documents\": documents if documents else state.get(\"documents\", [])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1114be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent creation helper function\n",
    "\n",
    "def create_agent(\n",
    "    llm: ChatOpenAI,\n",
    "    tools: list,\n",
    "    system_prompt: str,\n",
    ") -> str:\n",
    "    \"\"\"Create a function-calling agent and add it to the graph.\"\"\"\n",
    "    system_prompt += (\"\\nWork autonomously according to your specialty, using the tools available to you.\"\n",
    "    \" Do not ask for clarification.\"\n",
    "    \" Your other team members (and other teams) will collaborate with you with their own specialties.\"\n",
    "    \" You are chosen for a reason!\")\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                system_prompt,\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "        ]\n",
    "    )\n",
    "    agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "    executor = AgentExecutor(agent=agent, tools=tools, return_intermediate_steps=True)\n",
    "    return executor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13d34789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervisor Helper function\n",
    "def create_team_supervisor(llm: ChatOpenAI, system_prompt, members) -> str:\n",
    "    \"\"\"An LLM-based router.\"\"\"\n",
    "    options = [\"FINISH\"] + members\n",
    "    function_def = {\n",
    "        \"name\": \"route\",\n",
    "        \"description\": \"Select the next role.\",\n",
    "        \"parameters\": {\n",
    "            \"title\": \"routeSchema\",\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"next\": {\n",
    "                    \"title\": \"Next\",\n",
    "                    \"anyOf\": [\n",
    "                        {\"enum\": options},\n",
    "                    ],\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"next\"],\n",
    "        },\n",
    "    }\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            (\n",
    "                \"system\",\n",
    "                \"Given the conversation above, who should act next?\"\n",
    "                \" Or should we FINISH? Select one of: {options}\",\n",
    "            ),\n",
    "        ]\n",
    "    ).partial(options=str(options), team_members=\", \".join(members))\n",
    "    return (\n",
    "         prompt\n",
    "         | llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
    "         | JsonOutputFunctionsParser()\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fc6e92",
   "metadata": {},
   "source": [
    "#### Research Team - A LangGraph for Researching public sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "288dc429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to enhance the search with video specific information\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "summarization_llm = ChatOpenAI(model='gpt-4o-mini')\n",
    "\n",
    "SUMMARIZATION_TEMPLATE = (\n",
    "    \"You are an editorial analyst turning raw YouTube transcripts into concise research briefings.\\n\"\n",
    "    \"Video title: {title}\\n\"\n",
    "    \"Channel: {channel}\\n\"\n",
    "    \"Transcript excerpt:\\n{transcript}\\n\\n\"\n",
    "    \"Write a tight summary under 110 words that:\"\n",
    "    \"\\n- Captures the main topics and arguments\"\n",
    "    \"\\n- Names key people, brands, or entities mentioned\"\n",
    "    \"\\n- Notes tone shifts or controversies if present\"\n",
    "    \"\\n- Highlights any actionable insights for a researcher\"\n",
    "    \"\\nUse short sentences separated by semicolons. If the transcript excerpt is empty, respond with 'No transcript available.'\"\n",
    ")\n",
    "chat_summarization_prompt = ChatPromptTemplate.from_messages([(\"human\", SUMMARIZATION_TEMPLATE)])\n",
    "\n",
    "summarization_chain = chat_summarization_prompt | summarization_llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a5dcc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video context loaded:\n",
      "  Title: Using OpenAI Codex CLI with GPT-5-Codex\n",
      "  Channel: OpenAI\n",
      "  Comments available: 50\n"
     ]
    }
   ],
   "source": [
    "# Set up global variables for video_specific_search tool\n",
    "# These will be accessible via closure in the tool function\n",
    "\n",
    "title = unified_document.metadata.get(\"title\", \"\")\n",
    "channel = unified_document.metadata.get(\"channel\", \"\")\n",
    "transcript = unified_document.page_content  # Full unified document content\n",
    "# formatted_comments is already defined earlier from raw_blobs[\"formatted_comments\"]\n",
    "\n",
    "print(f\"Video context loaded:\")\n",
    "print(f\"  Title: {title}\")\n",
    "print(f\"  Channel: {channel}\")\n",
    "print(f\"  Comments available: {len(formatted_comments)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "988ae864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tavily search tool initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize Tavily search tool\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "tavily = TavilySearch(max_results=5)\n",
    "print(\"Tavily search tool initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef73e84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from typing import Annotated, List\n",
    "@tool\n",
    "def video_specific_search(\n",
    "    query: Annotated[str, \"Search query - can be about the current video's topic OR to find other related videos\"]\n",
    ") -> str:\n",
    "    \"\"\"Search for external information using web search (Tavily).\n",
    "    \n",
    "    Use this for:\n",
    "    - Finding other videos from the same creator/channel\n",
    "    - Searching for information about topics mentioned in the video\n",
    "    - Looking up external context related to the video's content\n",
    "    - General web searches enhanced with video context\n",
    "    \n",
    "    The search automatically includes the current video's title and channel for context.\n",
    "    \"\"\"\n",
    "    \n",
    "    search_context: List[str] = []\n",
    "    if title:\n",
    "        search_context.append(f'\"{title}\"')\n",
    "    if channel:\n",
    "        search_context.append(f\"channel:{channel}\")\n",
    "    if transcript:\n",
    "        summary = summarization_chain.invoke(\n",
    "            {\n",
    "                \"title\": title,\n",
    "                \"channel\": channel,\n",
    "                \"transcript\": transcript,\n",
    "            }\n",
    "        )\n",
    "        search_context.append(f\"transcript summary: {summary}\")\n",
    "\n",
    "    enhanced_query = f\"{query} {' '.join(search_context)}\" if search_context else query\n",
    "\n",
    "    results = tavily.invoke(enhanced_query)\n",
    "\n",
    "    output_lines = [f\"## Search Results for: {title or 'Unknown Video'}\"]\n",
    "    output_lines.append(f\"**Channel:** {channel or 'Unknown Channel'}\")\n",
    "    output_lines.append(f\"**Enhanced Query:** {enhanced_query}\\n\")\n",
    "\n",
    "    # Handle both list of dicts and list of strings from Tavily\n",
    "    for index, result in enumerate(results[:4], start=1):\n",
    "        output_lines.append(f\"### Result {index}\")\n",
    "        if isinstance(result, dict):\n",
    "            output_lines.append(f\"**URL:** {result.get('url', 'N/A')}\")\n",
    "            output_lines.append(f\"**Title:** {result.get('title', 'N/A')}\")\n",
    "            output_lines.append(\n",
    "                f\"**Summary:** {result.get('content', 'No content available')}\"\n",
    "            )\n",
    "        elif isinstance(result, str):\n",
    "            # If result is a string, just display it as content\n",
    "            output_lines.append(f\"**Content:** {result}\")\n",
    "        else:\n",
    "            output_lines.append(f\"**Result:** {str(result)}\")\n",
    "\n",
    "    return \"\\n\".join(output_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3b0711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a custom tool for retrieval that also stores documents globally\n",
    "from typing import Annotated\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Global variable to store documents (workaround for tool serialization)\n",
    "_retrieved_documents = []\n",
    "\n",
    "@tool\n",
    "def retrieve_information(\n",
    "    query: Annotated[str, \"query to ask the retrieve information tool\"]\n",
    "):\n",
    " \"\"\"Use Retrieval Augmented Generation to retrieve information related to user query\"\"\"\n",
    " global _retrieved_documents\n",
    " result = compiled_rag_graph.invoke({\"question\": query})\n",
    "\n",
    " _retrieved_documents = result['context']\n",
    " # Also return dict (though tool will serialize it to string)\n",
    " return {\n",
    "    \"response\" : result['response'],\n",
    "    \"documents\" : result['context']\n",
    " }   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c18dcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserah team state\n",
    "import functools\n",
    "import operator\n",
    "\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "import functools\n",
    "\n",
    "class ResearchTeamState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    documents: List[Document]  \n",
    "    team_members: List[str]\n",
    "    next: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6b72edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm for researching\n",
    "research_llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9ad710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Research Team: Video-Aware Search Agent + node\n",
    "\n",
    "search_agent = create_agent(\n",
    "    research_llm, \n",
    "    [video_specific_search], \n",
    "    \"\"\"You are a video-aware research assistant specialized in finding external information \n",
    "    related to the current video being analyzed. Your searches are automatically enhanced \n",
    "    with video context (title, channel, etc.) to provide more relevant results.\n",
    "    \n",
    "    When users ask about:\n",
    "    - Public opinion: Search for reactions, reviews, and discussions about this specific video\n",
    "    - Background info: Look for information about the video creator, topic, or related content\n",
    "    \n",
    "    Always focus your searches on content that would help understand sentiment and topics \n",
    "    in the video comments better.\"\"\"\n",
    ")\n",
    "search_node = functools.partial(agent_node, agent=search_agent, name='VideoSearch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c974df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Research Team: \n",
    "\n",
    "research_agent = create_agent(\n",
    "research_llm, \n",
    "[retrieve_information],  \n",
    "\"You are a research assistant who can retrieve and provide specific comments related to the query.\"\n",
    ")\n",
    "research_node = functools.partial(agent_node_with_docs, agent=research_agent, name= 'CommentFinder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16e31bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Inés\\AppData\\Local\\Temp\\ipykernel_274228\\3669758045.py:35: LangChainDeprecationWarning: The method `BaseChatOpenAI.bind_functions` was deprecated in langchain-openai 0.2.1 and will be removed in 1.0.0. Use :meth:`~langchain_openai.chat_models.base.ChatOpenAI.bind_tools` instead.\n",
      "  | llm.bind_functions(functions=[function_def], function_call=\"route\")\n"
     ]
    }
   ],
   "source": [
    "# Research Supervisor - Updated for Video-Specific Search\n",
    "research_supervisor_agent = create_team_supervisor(\n",
    "    research_llm,\n",
    "    (\"You are a supervisor tasked with managing a conversation between the\"\n",
    "    \" following workers: VideoSearch, CommentFinder. Given the following user request,\"\n",
    "    \" determine the subject to be researched and respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. \"\n",
    "    \" \"\n",
    "    \"VideoSearch: Use for external information related to the video (public opinions, background info, related content)\"\n",
    "    \"CommentFinder: Use for internal video data (comments, transcript, video details)\"\n",
    "    \" \"\n",
    "    \" You should never ask your team to do anything beyond research. They are not required to write content or posts.\"\n",
    "    \" You should only pass tasks to workers that are specifically research focused.\"\n",
    "    \" When finished, respond with FINISH.\"),\n",
    "    [\"VideoSearch\", \"CommentFinder\"],  # Updated team member names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3decad7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x26cef5f1890>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Research Team Graph Creation\n",
    "\n",
    "research_graph = StateGraph(ResearchTeamState)\n",
    "\n",
    "research_graph.add_node(\"VideoSearch\", search_node)  # Updated to VideoSearch\n",
    "research_graph.add_node(\"CommentFinder\", research_node)\n",
    "research_graph.add_node(\"ResearchSupervisor\", research_supervisor_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0b87694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x26cef5f1890>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "research_graph.add_edge(\"VideoSearch\", \"ResearchSupervisor\")  # Updated edge\n",
    "research_graph.add_edge(\"CommentFinder\", \"ResearchSupervisor\")\n",
    "research_graph.add_conditional_edges(\n",
    "    \"ResearchSupervisor\",\n",
    "    lambda x: x[\"next\"],\n",
    "    {\"VideoSearch\": \"VideoSearch\", \"CommentFinder\": \"CommentFinder\", \"FINISH\": END},  # Updated routing\n",
    ")\n",
    "research_graph.set_entry_point(\"ResearchSupervisor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a15c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_research_graph = research_graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc2a818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiled_research_graph #run if not timeouterror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "929fe35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a wrapper around compiled_research_graph to preserve the documents\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def research_chain_with_docs(state):\n",
    "    \"\"\"\n",
    "    Run research chain and explicitly preserve documents in state.\n",
    "   \n",
    "    \"\"\"\n",
    "    # Run the research graph\n",
    "    result = compiled_research_graph.invoke(state)\n",
    "    \n",
    "    # Explicitly extract and preserve documents\n",
    "    # (assuming they're stored somewhere in the result or state)\n",
    "    return {\n",
    "        \"messages\": result[\"messages\"],\n",
    "        \"documents\": result.get(\"documents\", state.get(\"documents\", []))\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e15476b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast messages down to Research Team LangGraph\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def enter_research_chain(message: str):\n",
    "    results = {\n",
    "        \"messages\": [HumanMessage(content=message)],\n",
    "        # \"documents\": [] # Initializing the documents\n",
    "    }\n",
    "    return results\n",
    "\n",
    "# Wrap functions in RunnableLambda to make them pipeable\n",
    "research_chain = (\n",
    "    RunnableLambda(enter_research_chain) \n",
    "    | RunnableLambda(research_chain_with_docs)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "09663874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What can you tell me about the overall video sentiment?', additional_kwargs={}, response_metadata={}), HumanMessage(content='The overall sentiment toward the video titled **\"Using OpenAI Codex CLI with GPT-5-Codex\"** appears to be positive. Comments indicate viewer interest and approval, with specific praise for Adam Driver’s performance from a user who stated, \"Adam Driver is great in this video.\" There are a few critical and humorous remarks about the content of the video and visual aspects, such as discrepancies between narration and visuals or the color scheme, but these are not predominant.\\n\\nThe video has garnered significant engagement, with **3,042 likes** and **265 comments**, suggesting that the audience finds it interesting and valuable. Overall, the sentiment indicates a favorable reception, reflecting enthusiasm for the content presented.', additional_kwargs={}, response_metadata={}, name='CommentFinder')], 'documents': [Document(metadata={'type': 'comment', 'comment_index': 36, 'author': '@xeoxaz', 'likes': 1, 'published': '2025-10-15T16:45:32Z', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', '_id': '152d6d7b0eb746698580527d604c420a', '_collection_name': 'video_sentiment_data'}, page_content='Is this whole video AI?'), Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified', '_id': '664212477d8c41a4a887161b2662a2ed', '_collection_name': 'video_sentiment_data'}, page_content='**Comment 34:**\\n- Author: @janosorcsik\\n- Likes: 0\\n- Published: 2025-10-15T17:03:26Z\\n- Text: Why is what the guy is saying different from what&#39;s shown in the video?\\n---\\n\\n**Comment 35:**\\n- Author: @repoles\\n- Likes: 0\\n- Published: 2025-10-15T16:57:11Z\\n- Text: My eyes hurt with this color scheme 😎\\n---\\n\\n**Comment 36:**\\n- Author: @xeoxaz\\n- Likes: 1\\n- Published: 2025-10-15T16:45:32Z\\n- Text: Is this whole video AI?\\n---'), Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified', '_id': '5e4648f75d1247b2ba9f190b279df0ac', '_collection_name': 'video_sentiment_data'}, page_content='## Video Transcript'), Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified', '_id': 'b13ff63d71d3432c982488778f3a612b', '_collection_name': 'video_sentiment_data'}, page_content='**Comment 19:**\\n- Author: @endoflevelboss\\n- Likes: 0\\n- Published: 2025-10-16T08:10:49Z\\n- Text: How did that guy&#39;s haircut be allowed on camera?\\n---\\n\\n**Comment 20:**\\n- Author: @stym06\\n- Likes: 1\\n- Published: 2025-10-16T04:58:35Z\\n- Text: Adam Driver is great in this video\\n---\\n\\n**Comment 21:**\\n- Author: @outsidefive\\n- Likes: 0\\n- Published: 2025-10-16T03:56:19Z\\n- Text: Hey why is it not working? I don&#39;t know, ask GPT. Rip Software Engineering.\\n---\\n\\n**Comment 22:**\\n- Author: @AnonDev77\\n- Likes: 0\\n- Published: 2025-10-16T02:17:56Z\\n- Text: get Eson(?) on more of these videos.  thumbs up. loving codex\\n---'), Document(metadata={'type': 'comment', 'comment_index': 34, 'author': '@janosorcsik', 'likes': 0, 'published': '2025-10-15T17:03:26Z', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', '_id': 'fd56babaed2141bdb7844d6fa065d387', '_collection_name': 'video_sentiment_data'}, page_content='Why is what the guy is saying different from what&#39;s shown in the video?'), Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified', '_id': 'fb020d2f3b964739b615a0eec8e4e334', '_collection_name': 'video_sentiment_data'}, page_content='# VIDEO ANALYSIS CONTEXT\\n\\n## Video Information\\n**Title:** Using OpenAI Codex CLI with GPT-5-Codex\\n**Channel:** OpenAI\\n**Published:** 2025-10-14T18:19:27Z\\n**Views:** 134177\\n**Likes:** 3042\\n**Comments Count:** 265\\n\\n## Video Description\\nLearn how to use the Codex CLI + GPT-5-Codex in just 5 minutes.\\n\\n\\nIn this tutorial, Eason Goodale and Romain Huet show you how to build a multiplayer game with Codex — without writing a single line of code! \\n\\nTimestamps:\\n\\n00:00 Intro\\n00:50 Planning the multiplayer implementation\\n1:38 Using CLI commands\\n2:55 Implementing the plan\\n3:31 Deploy the game\\n4:20 Playing the game\\n\\n\\nTo learn more and get started:')]}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for s in research_chain.stream(\n",
    "    \"What can you tell me about the overall video sentiment?\", {\"recursion_limit\": 100}\n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87378939",
   "metadata": {},
   "source": [
    "#### Sentiment-Topic Analysis team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ae767842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defininf the tools\n",
    "\n",
    "@tool(description=\"Sentiment analysis reflection tool for quality decision-making\")\n",
    "def sentiment_think_tool(reflection: str) -> str:\n",
    "    \"\"\"Tool for strategic reflection during sentiment analysis workflows.\n",
    "\n",
    "    Use this tool to pause and reflect on sentiment analysis progress, ensuring high-quality insights.\n",
    "\n",
    "    When to use:\n",
    "    - After processing comment batches: What sentiment patterns emerged?\n",
    "    - Before finalizing analysis: Do I have sufficient data for reliable conclusions?\n",
    "    - When encountering mixed sentiments: How should I categorize ambiguous cases?\n",
    "    - Before generating reports: Are my sentiment classifications well-supported?\n",
    "\n",
    "    Reflection should address:\n",
    "    1. Sentiment pattern recognition - What dominant sentiments did I identify?\n",
    "    2. Data quality assessment - Are the comments representative and sufficient?\n",
    "    3. Classification confidence - How certain am I about sentiment labels?\n",
    "    4. Analysis completeness - Do I need more data or can I proceed with conclusions?\n",
    "\n",
    "    Args:\n",
    "        reflection: Your detailed reflection on sentiment analysis findings, patterns, and next steps\n",
    "\n",
    "    Returns:\n",
    "        Confirmation that sentiment analysis reflection was recorded\n",
    "    \"\"\"\n",
    "    return f\"Sentiment analysis reflection recorded: {reflection}\"\n",
    "\n",
    "\n",
    "@tool(description=\"Topic extraction reflection tool for quality categorization\")\n",
    "def topic_think_tool(reflection: str) -> str:\n",
    "    \"\"\"Tool for strategic reflection during topic extraction workflows.\n",
    "\n",
    "    Use this tool to pause and reflect on topic discovery progress, ensuring comprehensive theme identification.\n",
    "\n",
    "    When to use:\n",
    "    - After processing content batches: What topics and themes emerged?\n",
    "    - Before finalizing categorization: Do I have sufficient examples for each topic?\n",
    "    - When encountering ambiguous content: How should I categorize unclear topics?\n",
    "    - Before generating topic reports: Are my topic classifications well-supported?\n",
    "\n",
    "    Reflection should address:\n",
    "    1. Topic discovery - What distinct themes and subjects did I identify?\n",
    "    2. Topic coverage - Do I have enough examples for reliable topic classification?\n",
    "    3. Classification confidence - How certain am I about topic labels and boundaries?\n",
    "    4. Analysis completeness - Do I need more content or can I proceed with topic conclusions?\n",
    "\n",
    "    Args:\n",
    "        reflection: Your detailed reflection on topic extraction findings, themes, and next steps\n",
    "\n",
    "    Returns:\n",
    "        Confirmation that topic extraction reflection was recorded\n",
    "    \"\"\"\n",
    "    return f\"Topic extraction reflection recorded: {reflection}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "db61ab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Team state\n",
    "\n",
    "import operator\n",
    "\n",
    "class SentimentState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    documents: List[Document]\n",
    "    team_members: str\n",
    "    next: str\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d1ad3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "be96b531",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_agent = create_agent(\n",
    "    analysis_llm,\n",
    "    [topic_think_tool],\n",
    "    (\"You are an expert at topic extraction\"),\n",
    ")\n",
    "\n",
    "topic_node = functools.partial(agent_node, agent=topic_agent, name='Topic')\n",
    "\n",
    "sentiment_agent = create_agent(\n",
    "    analysis_llm,\n",
    "    [sentiment_think_tool],\n",
    "    (\"You are an expert at sentiment analysis\")\n",
    ")\n",
    "\n",
    "sentiment_node = functools.partial(agent_node, agent=sentiment_agent, name='Sentiment')\n",
    "\n",
    "\n",
    "analysis_supervisor_agent = create_team_supervisor(\n",
    "    analysis_llm,\n",
    "    (\"You are a supervisor tasked with managing a conversation between the\"\n",
    "    \" following workers: {team_members}. You should always verify the analysis\"\n",
    "    \" contents after any decision is been made. \"\n",
    "    \"Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. When each team is finished,\"\n",
    "    \" you must respond with FINISH.\"),\n",
    "    [\"Sentiment\", \"Topic\"],\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a2743870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x26cf0d7f290>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_graph = StateGraph(SentimentState)\n",
    "sentiment_graph.add_node(\"Topic\", topic_node)\n",
    "sentiment_graph.add_node(\"Sentiment\", sentiment_node)\n",
    "sentiment_graph.add_node(\"AnalysisSupervisor\", analysis_supervisor_agent)\n",
    "\n",
    "sentiment_graph.add_edge(\"Topic\", \"AnalysisSupervisor\")\n",
    "sentiment_graph.add_edge(\"Sentiment\", \"AnalysisSupervisor\")\n",
    "\n",
    "\n",
    "sentiment_graph.add_conditional_edges(\n",
    "    \"AnalysisSupervisor\",\n",
    "    lambda x: x[\"next\"],\n",
    "    {\n",
    "        \"Topic\": \"Topic\",\n",
    "        \"Sentiment\": \"Sentiment\",\n",
    "        \"FINISH\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "sentiment_graph.set_entry_point(\"AnalysisSupervisor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5857c1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_sentiment_graph = sentiment_graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "173a4e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x00000223A81E27A0>"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compiled_sentiment_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3dcedd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enter_sentiment_chain(state: dict, members: List[str]):\n",
    "    \"\"\"Prepare state for sentiment analysis team\"\"\"\n",
    "    results = {\n",
    "        \"messages\": [HumanMessage(content=state[\"message\"])],\n",
    "        \"documents\": state.get(\"documents\", []),\n",
    "        \"team_members\": \", \".join(members),\n",
    "    }\n",
    "    return results\n",
    "\n",
    "# Use RunnableLambda for proper LCEL composition\n",
    "analysis_chain = (\n",
    "    RunnableLambda(functools.partial(enter_sentiment_chain, members=sentiment_graph.nodes))\n",
    "    | compiled_sentiment_graph\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4911c960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AnalysisSupervisor': {'next': 'Sentiment'}}\n",
      "---\n",
      "{'Sentiment': {'messages': [HumanMessage(content='Please provide the comments you would like me to analyze for sentiment patterns.', additional_kwargs={}, response_metadata={}, name='Sentiment')], 'documents': [Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified'}, page_content='# VIDEO ANALYSIS CONTEXT\\n\\n## Video Information\\n**Title:** Using OpenAI Codex CLI with GPT-5-Codex\\n**Channel:** OpenAI\\n**Published:** 2025-10-14T18:19:27Z\\n**Views:** 134177\\n**Likes:** 3042\\n**Comments Count:** 265\\n\\n## Video Description\\nLearn how to use the Codex CLI + GPT-5-Codex in just 5 minutes.\\n\\n\\nIn this tutorial, Eason Goodale and Romain Huet show you how to build a multiplayer game with Codex — without writing a single line of code! \\n\\nTimestamps:\\n\\n00:00 Intro\\n00:50 Planning the multiplayer implementation\\n1:38 Using CLI commands\\n2:55 Implementing the plan\\n3:31 Deploy the game\\n4:20 Playing the game\\n\\n\\nTo learn more and get started:'), Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified'}, page_content='To learn more and get started:\\n\\nCodex: openai.com/codex \\nCodex CLI GitHub repo: https://github.com/openai/codex \\nDeveloper docs: developers.openai.com/codex'), Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified'}, page_content='## Video Transcript'), Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified'}, page_content=\"Hey, what are you working on? >> I'm trying to find a good demo. Something that Codex can modify. We could make this little ball thing multiplayer. >> That sounds very cool. Let's do it. >> Codeex CL164. Mark. >> Hey everyone, Roma here. Recently, we ship GP5 and GP5 codecs and we've also released a ton of improvements to Codex CLI to better harness the agentic coding capabilities of these models. And today I'm sitting with Eson who led a lot of this effort on the CLI. Do you want to give us a quick tour? >> Yeah, I'd love to. Um, we have tons of really cool updates. You can install it really easily with either mpm or brew and log in with your chat GPT account. >> So here you're in your terminal and you just have to launch it with by\"), Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified'}, page_content=\"easily with either mpm or brew and log in with your chat GPT account. >> So here you're in your terminal and you just have to launch it with by codex. >> That's all there is to it. So we'll say make a plan for making this game multiplayer. What's funny is like this game was one of the very many examples we shipped completely built by GPT5 in one prompt. >> Yes. >> Like and now we can start building up upon it. So what it's thinking tell us a little more like about what's happening which model you're using here. >> Yeah. So this is uh going to be GPT5 codeex which is our new model and it's really good for any sort of coding task. >> So here it's like currently crafting the plan. I see it's laying out the steps of what it's supposed to do.\")]}}\n",
      "---\n",
      "{'Sentiment': {'messages': [HumanMessage(content='Please provide the comments you would like me to analyze for sentiment patterns.', additional_kwargs={}, response_metadata={}, name='Sentiment')], 'documents': [Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified'}, page_content='# VIDEO ANALYSIS CONTEXT\\n\\n## Video Information\\n**Title:** Using OpenAI Codex CLI with GPT-5-Codex\\n**Channel:** OpenAI\\n**Published:** 2025-10-14T18:19:27Z\\n**Views:** 134177\\n**Likes:** 3042\\n**Comments Count:** 265\\n\\n## Video Description\\nLearn how to use the Codex CLI + GPT-5-Codex in just 5 minutes.\\n\\n\\nIn this tutorial, Eason Goodale and Romain Huet show you how to build a multiplayer game with Codex — without writing a single line of code! \\n\\nTimestamps:\\n\\n00:00 Intro\\n00:50 Planning the multiplayer implementation\\n1:38 Using CLI commands\\n2:55 Implementing the plan\\n3:31 Deploy the game\\n4:20 Playing the game\\n\\n\\nTo learn more and get started:'), Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified'}, page_content='To learn more and get started:\\n\\nCodex: openai.com/codex \\nCodex CLI GitHub repo: https://github.com/openai/codex \\nDeveloper docs: developers.openai.com/codex'), Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified'}, page_content='## Video Transcript'), Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified'}, page_content=\"Hey, what are you working on? >> I'm trying to find a good demo. Something that Codex can modify. We could make this little ball thing multiplayer. >> That sounds very cool. Let's do it. >> Codeex CL164. Mark. >> Hey everyone, Roma here. Recently, we ship GP5 and GP5 codecs and we've also released a ton of improvements to Codex CLI to better harness the agentic coding capabilities of these models. And today I'm sitting with Eson who led a lot of this effort on the CLI. Do you want to give us a quick tour? >> Yeah, I'd love to. Um, we have tons of really cool updates. You can install it really easily with either mpm or brew and log in with your chat GPT account. >> So here you're in your terminal and you just have to launch it with by\"), Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified'}, page_content=\"easily with either mpm or brew and log in with your chat GPT account. >> So here you're in your terminal and you just have to launch it with by codex. >> That's all there is to it. So we'll say make a plan for making this game multiplayer. What's funny is like this game was one of the very many examples we shipped completely built by GPT5 in one prompt. >> Yes. >> Like and now we can start building up upon it. So what it's thinking tell us a little more like about what's happening which model you're using here. >> Yeah. So this is uh going to be GPT5 codeex which is our new model and it's really good for any sort of coding task. >> So here it's like currently crafting the plan. I see it's laying out the steps of what it's supposed to do.\")]}}\n",
      "---\n",
      "{'AnalysisSupervisor': {'next': 'Topic'}}\n",
      "---\n",
      "{'AnalysisSupervisor': {'next': 'Topic'}}\n",
      "---\n",
      "{'Topic': {'messages': [HumanMessage(content=\"I need the specific comments you'd like analyzed for sentiment patterns in order to proceed. Please provide them.\", additional_kwargs={}, response_metadata={}, name='Topic')], 'documents': [Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified'}, page_content='# VIDEO ANALYSIS CONTEXT\\n\\n## Video Information\\n**Title:** Using OpenAI Codex CLI with GPT-5-Codex\\n**Channel:** OpenAI\\n**Published:** 2025-10-14T18:19:27Z\\n**Views:** 134177\\n**Likes:** 3042\\n**Comments Count:** 265\\n\\n## Video Description\\nLearn how to use the Codex CLI + GPT-5-Codex in just 5 minutes.\\n\\n\\nIn this tutorial, Eason Goodale and Romain Huet show you how to build a multiplayer game with Codex — without writing a single line of code! \\n\\nTimestamps:\\n\\n00:00 Intro\\n00:50 Planning the multiplayer implementation\\n1:38 Using CLI commands\\n2:55 Implementing the plan\\n3:31 Deploy the game\\n4:20 Playing the game\\n\\n\\nTo learn more and get started:'), Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified'}, page_content='To learn more and get started:\\n\\nCodex: openai.com/codex \\nCodex CLI GitHub repo: https://github.com/openai/codex \\nDeveloper docs: developers.openai.com/codex'), Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified'}, page_content='## Video Transcript'), Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified'}, page_content=\"Hey, what are you working on? >> I'm trying to find a good demo. Something that Codex can modify. We could make this little ball thing multiplayer. >> That sounds very cool. Let's do it. >> Codeex CL164. Mark. >> Hey everyone, Roma here. Recently, we ship GP5 and GP5 codecs and we've also released a ton of improvements to Codex CLI to better harness the agentic coding capabilities of these models. And today I'm sitting with Eson who led a lot of this effort on the CLI. Do you want to give us a quick tour? >> Yeah, I'd love to. Um, we have tons of really cool updates. You can install it really easily with either mpm or brew and log in with your chat GPT account. >> So here you're in your terminal and you just have to launch it with by\"), Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified'}, page_content=\"easily with either mpm or brew and log in with your chat GPT account. >> So here you're in your terminal and you just have to launch it with by codex. >> That's all there is to it. So we'll say make a plan for making this game multiplayer. What's funny is like this game was one of the very many examples we shipped completely built by GPT5 in one prompt. >> Yes. >> Like and now we can start building up upon it. So what it's thinking tell us a little more like about what's happening which model you're using here. >> Yeah. So this is uh going to be GPT5 codeex which is our new model and it's really good for any sort of coding task. >> So here it's like currently crafting the plan. I see it's laying out the steps of what it's supposed to do.\")]}}\n",
      "---\n",
      "{'Topic': {'messages': [HumanMessage(content=\"I need the specific comments you'd like analyzed for sentiment patterns in order to proceed. Please provide them.\", additional_kwargs={}, response_metadata={}, name='Topic')], 'documents': [Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified'}, page_content='# VIDEO ANALYSIS CONTEXT\\n\\n## Video Information\\n**Title:** Using OpenAI Codex CLI with GPT-5-Codex\\n**Channel:** OpenAI\\n**Published:** 2025-10-14T18:19:27Z\\n**Views:** 134177\\n**Likes:** 3042\\n**Comments Count:** 265\\n\\n## Video Description\\nLearn how to use the Codex CLI + GPT-5-Codex in just 5 minutes.\\n\\n\\nIn this tutorial, Eason Goodale and Romain Huet show you how to build a multiplayer game with Codex — without writing a single line of code! \\n\\nTimestamps:\\n\\n00:00 Intro\\n00:50 Planning the multiplayer implementation\\n1:38 Using CLI commands\\n2:55 Implementing the plan\\n3:31 Deploy the game\\n4:20 Playing the game\\n\\n\\nTo learn more and get started:'), Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified'}, page_content='To learn more and get started:\\n\\nCodex: openai.com/codex \\nCodex CLI GitHub repo: https://github.com/openai/codex \\nDeveloper docs: developers.openai.com/codex'), Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified'}, page_content='## Video Transcript'), Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified'}, page_content=\"Hey, what are you working on? >> I'm trying to find a good demo. Something that Codex can modify. We could make this little ball thing multiplayer. >> That sounds very cool. Let's do it. >> Codeex CL164. Mark. >> Hey everyone, Roma here. Recently, we ship GP5 and GP5 codecs and we've also released a ton of improvements to Codex CLI to better harness the agentic coding capabilities of these models. And today I'm sitting with Eson who led a lot of this effort on the CLI. Do you want to give us a quick tour? >> Yeah, I'd love to. Um, we have tons of really cool updates. You can install it really easily with either mpm or brew and log in with your chat GPT account. >> So here you're in your terminal and you just have to launch it with by\"), Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified'}, page_content=\"easily with either mpm or brew and log in with your chat GPT account. >> So here you're in your terminal and you just have to launch it with by codex. >> That's all there is to it. So we'll say make a plan for making this game multiplayer. What's funny is like this game was one of the very many examples we shipped completely built by GPT5 in one prompt. >> Yes. >> Like and now we can start building up upon it. So what it's thinking tell us a little more like about what's happening which model you're using here. >> Yeah. So this is uh going to be GPT5 codeex which is our new model and it's really good for any sort of coding task. >> So here it's like currently crafting the plan. I see it's laying out the steps of what it's supposed to do.\")]}}\n",
      "---\n",
      "{'AnalysisSupervisor': {'next': 'FINISH'}}\n",
      "---\n",
      "{'AnalysisSupervisor': {'next': 'FINISH'}}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "#test analysis chain: we provide a sample of documents as it is design to work inside the super_graph\n",
    "\n",
    "test_documents = docs_for_store[:5]  \n",
    "\n",
    "# Call analysis_chain with both message and documents\n",
    "for s in compiled_sentiment_graph.stream(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"What are the main sentiment patterns in these comments?\")],\n",
    "        \"documents\": test_documents,\n",
    "        \"team_members\": \", \".join(sentiment_graph.nodes)\n",
    "    },\n",
    "    {\"recursion_limit\": 100},\n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bad5fc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Meta-Supervisor and full graph\n",
    "\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "super_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "super_supervisor_agent = create_team_supervisor(\n",
    "    super_llm,\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    \" following teams: {team_members}. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. When all workers are finished,\"\n",
    "    \" you must respond with FINISH.\",\n",
    "    [\"Research team\", \"Analysis team\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "56e52264",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    documents: List[Document]  \n",
    "    next: str\n",
    "\n",
    "def get_last_message(state: State) -> str:\n",
    "    return state[\"messages\"][-1].content\n",
    "\n",
    "\n",
    "def join_graph(response: dict):\n",
    "    result = {\"messages\": [response[\"messages\"][-1]]}\n",
    "    if \"documents\" in response and response[\"documents\"]:\n",
    "        result[\"documents\"] = response[\"documents\"]\n",
    "    return result\n",
    "\n",
    "def get_messages_and_documents(state: State):\n",
    "    # Extract the last message content and documents\n",
    "    message = state[\"messages\"][-1].content\n",
    "    documents = state.get(\"documents\", [])\n",
    "    # Call analysis_chain and join_graph in one function\n",
    "    result = analysis_chain.invoke({\"message\": message, \"documents\": documents})\n",
    "    return join_graph(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ffdd65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x26cf0e7c410>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "super_graph = StateGraph(State)\n",
    "\n",
    "super_graph.add_node(\"Research team\", get_last_message | research_chain | join_graph)\n",
    "super_graph.add_node(\"Analysis team\", get_messages_and_documents)\n",
    "super_graph.add_node(\"SuperSupervisor\", super_supervisor_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5bd2dc1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x26cf0e7c410>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "super_graph.add_edge(\"Research team\", \"SuperSupervisor\")\n",
    "super_graph.add_edge(\"Analysis team\", \"SuperSupervisor\")\n",
    "super_graph.add_conditional_edges(\n",
    "    \"SuperSupervisor\",\n",
    "    lambda x: x[\"next\"],\n",
    "    {\n",
    "        \"Analysis team\": \"Analysis team\",\n",
    "        \"Research team\": \"Research team\",\n",
    "        \"FINISH\": END,\n",
    "    },\n",
    ")\n",
    "super_graph.set_entry_point(\"SuperSupervisor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "babc6ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_super_graph = super_graph.compile()\n",
    "# compiled_super_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "51e7a56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SuperSupervisor': {'next': 'Research team'}}\n",
      "---\n",
      "{'Research team': {'messages': [HumanMessage(content='The comments on the video titled \"Using OpenAI Codex CLI with GPT-5-Codex\" exhibit a generally neutral to mildly positive sentiment, with many users showing curiosity, some skepticism, and a bit of humor. \\n\\n**Key Topics and Sentiments:**\\n\\n1. **Technical Curiosity**: \\n   - Users are interested in understanding the technical distinctions in the content; e.g., one comment asks why what\\'s said in the video differs from what\\'s shown.\\n   - A user questions the nature of the video itself, wondering whether it’s entirely AI-generated.\\n\\n2. **Appreciation for Content**: \\n   - Comments express positive sentiments about elements within the video, with appreciation for figures like Adam Driver contributing positively, e.g., discussing his performance.\\n\\n3. **Visual Presentation**: \\n   - Comments critique the visual aspects, such as a user indicating that the color scheme is uncomfortable, highlighting a mixed experience of watching.\\n\\n4. **Skepticism Regarding Engagement**: \\n   - Some users express doubt over the originality of comments, suggesting that they may appear bot-like or inauthentic.\\n\\nOverall, the sentiment leans toward curiosity and positive engagement centered on the AI technology and humorous or observational remarks about the visual presentation and content.', additional_kwargs={}, response_metadata={}, name='CommentFinder')], 'documents': [Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified', '_id': '664212477d8c41a4a887161b2662a2ed', '_collection_name': 'video_sentiment_data'}, page_content='**Comment 34:**\\n- Author: @janosorcsik\\n- Likes: 0\\n- Published: 2025-10-15T17:03:26Z\\n- Text: Why is what the guy is saying different from what&#39;s shown in the video?\\n---\\n\\n**Comment 35:**\\n- Author: @repoles\\n- Likes: 0\\n- Published: 2025-10-15T16:57:11Z\\n- Text: My eyes hurt with this color scheme 😎\\n---\\n\\n**Comment 36:**\\n- Author: @xeoxaz\\n- Likes: 1\\n- Published: 2025-10-15T16:45:32Z\\n- Text: Is this whole video AI?\\n---'), Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified', '_id': 'b13ff63d71d3432c982488778f3a612b', '_collection_name': 'video_sentiment_data'}, page_content='**Comment 19:**\\n- Author: @endoflevelboss\\n- Likes: 0\\n- Published: 2025-10-16T08:10:49Z\\n- Text: How did that guy&#39;s haircut be allowed on camera?\\n---\\n\\n**Comment 20:**\\n- Author: @stym06\\n- Likes: 1\\n- Published: 2025-10-16T04:58:35Z\\n- Text: Adam Driver is great in this video\\n---\\n\\n**Comment 21:**\\n- Author: @outsidefive\\n- Likes: 0\\n- Published: 2025-10-16T03:56:19Z\\n- Text: Hey why is it not working? I don&#39;t know, ask GPT. Rip Software Engineering.\\n---\\n\\n**Comment 22:**\\n- Author: @AnonDev77\\n- Likes: 0\\n- Published: 2025-10-16T02:17:56Z\\n- Text: get Eson(?) on more of these videos.  thumbs up. loving codex\\n---'), Document(metadata={'type': 'comment', 'comment_index': 34, 'author': '@janosorcsik', 'likes': 0, 'published': '2025-10-15T17:03:26Z', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', '_id': 'fd56babaed2141bdb7844d6fa065d387', '_collection_name': 'video_sentiment_data'}, page_content='Why is what the guy is saying different from what&#39;s shown in the video?'), Document(metadata={'type': 'comment', 'comment_index': 27, 'author': '@dandragomir6413', 'likes': 0, 'published': '2025-10-16T00:23:30Z', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', '_id': '3b479fbeef424023b61a56c8fd770b9d', '_collection_name': 'video_sentiment_data'}, page_content='are all comments here written by bots? thats what it seems like'), Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified', '_id': '5e4648f75d1247b2ba9f190b279df0ac', '_collection_name': 'video_sentiment_data'}, page_content='## Video Transcript'), Document(metadata={'type': 'comment', 'comment_index': 36, 'author': '@xeoxaz', 'likes': 1, 'published': '2025-10-15T16:45:32Z', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', '_id': '152d6d7b0eb746698580527d604c420a', '_collection_name': 'video_sentiment_data'}, page_content='Is this whole video AI?')]}}\n",
      "---\n",
      "{'Research team': {'messages': [HumanMessage(content='The comments on the video titled \"Using OpenAI Codex CLI with GPT-5-Codex\" exhibit a generally neutral to mildly positive sentiment, with many users showing curiosity, some skepticism, and a bit of humor. \\n\\n**Key Topics and Sentiments:**\\n\\n1. **Technical Curiosity**: \\n   - Users are interested in understanding the technical distinctions in the content; e.g., one comment asks why what\\'s said in the video differs from what\\'s shown.\\n   - A user questions the nature of the video itself, wondering whether it’s entirely AI-generated.\\n\\n2. **Appreciation for Content**: \\n   - Comments express positive sentiments about elements within the video, with appreciation for figures like Adam Driver contributing positively, e.g., discussing his performance.\\n\\n3. **Visual Presentation**: \\n   - Comments critique the visual aspects, such as a user indicating that the color scheme is uncomfortable, highlighting a mixed experience of watching.\\n\\n4. **Skepticism Regarding Engagement**: \\n   - Some users express doubt over the originality of comments, suggesting that they may appear bot-like or inauthentic.\\n\\nOverall, the sentiment leans toward curiosity and positive engagement centered on the AI technology and humorous or observational remarks about the visual presentation and content.', additional_kwargs={}, response_metadata={}, name='CommentFinder')], 'documents': [Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified', '_id': '664212477d8c41a4a887161b2662a2ed', '_collection_name': 'video_sentiment_data'}, page_content='**Comment 34:**\\n- Author: @janosorcsik\\n- Likes: 0\\n- Published: 2025-10-15T17:03:26Z\\n- Text: Why is what the guy is saying different from what&#39;s shown in the video?\\n---\\n\\n**Comment 35:**\\n- Author: @repoles\\n- Likes: 0\\n- Published: 2025-10-15T16:57:11Z\\n- Text: My eyes hurt with this color scheme 😎\\n---\\n\\n**Comment 36:**\\n- Author: @xeoxaz\\n- Likes: 1\\n- Published: 2025-10-15T16:45:32Z\\n- Text: Is this whole video AI?\\n---'), Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified', '_id': 'b13ff63d71d3432c982488778f3a612b', '_collection_name': 'video_sentiment_data'}, page_content='**Comment 19:**\\n- Author: @endoflevelboss\\n- Likes: 0\\n- Published: 2025-10-16T08:10:49Z\\n- Text: How did that guy&#39;s haircut be allowed on camera?\\n---\\n\\n**Comment 20:**\\n- Author: @stym06\\n- Likes: 1\\n- Published: 2025-10-16T04:58:35Z\\n- Text: Adam Driver is great in this video\\n---\\n\\n**Comment 21:**\\n- Author: @outsidefive\\n- Likes: 0\\n- Published: 2025-10-16T03:56:19Z\\n- Text: Hey why is it not working? I don&#39;t know, ask GPT. Rip Software Engineering.\\n---\\n\\n**Comment 22:**\\n- Author: @AnonDev77\\n- Likes: 0\\n- Published: 2025-10-16T02:17:56Z\\n- Text: get Eson(?) on more of these videos.  thumbs up. loving codex\\n---'), Document(metadata={'type': 'comment', 'comment_index': 34, 'author': '@janosorcsik', 'likes': 0, 'published': '2025-10-15T17:03:26Z', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', '_id': 'fd56babaed2141bdb7844d6fa065d387', '_collection_name': 'video_sentiment_data'}, page_content='Why is what the guy is saying different from what&#39;s shown in the video?'), Document(metadata={'type': 'comment', 'comment_index': 27, 'author': '@dandragomir6413', 'likes': 0, 'published': '2025-10-16T00:23:30Z', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', '_id': '3b479fbeef424023b61a56c8fd770b9d', '_collection_name': 'video_sentiment_data'}, page_content='are all comments here written by bots? thats what it seems like'), Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified', '_id': '5e4648f75d1247b2ba9f190b279df0ac', '_collection_name': 'video_sentiment_data'}, page_content='## Video Transcript'), Document(metadata={'type': 'comment', 'comment_index': 36, 'author': '@xeoxaz', 'likes': 1, 'published': '2025-10-15T16:45:32Z', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', '_id': '152d6d7b0eb746698580527d604c420a', '_collection_name': 'video_sentiment_data'}, page_content='Is this whole video AI?')]}}\n",
      "---\n",
      "{'SuperSupervisor': {'next': 'Analysis team'}}\n",
      "---\n",
      "{'SuperSupervisor': {'next': 'Analysis team'}}\n",
      "---\n",
      "{'Analysis team': {'messages': [HumanMessage(content='The topic extraction reflection has been successfully recorded. Here are the key topics identified from the sentiment analysis:\\n\\n1. **Technical Curiosity**: Users are eager to delve into the technical details presented in the video, questioning discrepancies and the AI-generated nature of the content.\\n\\n2. **Appreciation for Contributors**: Positive sentiments are expressed regarding the contributions of individuals like Adam Driver, indicating an overall admiration for the content and performances.\\n\\n3. **Visual Critique**: Feedback includes critiques of the visual presentation, emphasizing the discomfort caused by elements like the color scheme.\\n\\n4. **Skepticism of Engagement**: Some users express skepticism about the authenticity of comments, suggesting a possibility of bot-like interactions.\\n\\nOverall, the feedback demonstrates a blend of curiosity, appreciation, and critical observation regarding the video and its content.', additional_kwargs={}, response_metadata={}, name='Topic')], 'documents': [Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified', '_id': '664212477d8c41a4a887161b2662a2ed', '_collection_name': 'video_sentiment_data'}, page_content='**Comment 34:**\\n- Author: @janosorcsik\\n- Likes: 0\\n- Published: 2025-10-15T17:03:26Z\\n- Text: Why is what the guy is saying different from what&#39;s shown in the video?\\n---\\n\\n**Comment 35:**\\n- Author: @repoles\\n- Likes: 0\\n- Published: 2025-10-15T16:57:11Z\\n- Text: My eyes hurt with this color scheme 😎\\n---\\n\\n**Comment 36:**\\n- Author: @xeoxaz\\n- Likes: 1\\n- Published: 2025-10-15T16:45:32Z\\n- Text: Is this whole video AI?\\n---'), Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified', '_id': 'b13ff63d71d3432c982488778f3a612b', '_collection_name': 'video_sentiment_data'}, page_content='**Comment 19:**\\n- Author: @endoflevelboss\\n- Likes: 0\\n- Published: 2025-10-16T08:10:49Z\\n- Text: How did that guy&#39;s haircut be allowed on camera?\\n---\\n\\n**Comment 20:**\\n- Author: @stym06\\n- Likes: 1\\n- Published: 2025-10-16T04:58:35Z\\n- Text: Adam Driver is great in this video\\n---\\n\\n**Comment 21:**\\n- Author: @outsidefive\\n- Likes: 0\\n- Published: 2025-10-16T03:56:19Z\\n- Text: Hey why is it not working? I don&#39;t know, ask GPT. Rip Software Engineering.\\n---\\n\\n**Comment 22:**\\n- Author: @AnonDev77\\n- Likes: 0\\n- Published: 2025-10-16T02:17:56Z\\n- Text: get Eson(?) on more of these videos.  thumbs up. loving codex\\n---'), Document(metadata={'type': 'comment', 'comment_index': 34, 'author': '@janosorcsik', 'likes': 0, 'published': '2025-10-15T17:03:26Z', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', '_id': 'fd56babaed2141bdb7844d6fa065d387', '_collection_name': 'video_sentiment_data'}, page_content='Why is what the guy is saying different from what&#39;s shown in the video?'), Document(metadata={'type': 'comment', 'comment_index': 27, 'author': '@dandragomir6413', 'likes': 0, 'published': '2025-10-16T00:23:30Z', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', '_id': '3b479fbeef424023b61a56c8fd770b9d', '_collection_name': 'video_sentiment_data'}, page_content='are all comments here written by bots? thats what it seems like'), Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified', '_id': '5e4648f75d1247b2ba9f190b279df0ac', '_collection_name': 'video_sentiment_data'}, page_content='## Video Transcript'), Document(metadata={'type': 'comment', 'comment_index': 36, 'author': '@xeoxaz', 'likes': 1, 'published': '2025-10-15T16:45:32Z', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', '_id': '152d6d7b0eb746698580527d604c420a', '_collection_name': 'video_sentiment_data'}, page_content='Is this whole video AI?')]}}\n",
      "---\n",
      "{'Analysis team': {'messages': [HumanMessage(content='The topic extraction reflection has been successfully recorded. Here are the key topics identified from the sentiment analysis:\\n\\n1. **Technical Curiosity**: Users are eager to delve into the technical details presented in the video, questioning discrepancies and the AI-generated nature of the content.\\n\\n2. **Appreciation for Contributors**: Positive sentiments are expressed regarding the contributions of individuals like Adam Driver, indicating an overall admiration for the content and performances.\\n\\n3. **Visual Critique**: Feedback includes critiques of the visual presentation, emphasizing the discomfort caused by elements like the color scheme.\\n\\n4. **Skepticism of Engagement**: Some users express skepticism about the authenticity of comments, suggesting a possibility of bot-like interactions.\\n\\nOverall, the feedback demonstrates a blend of curiosity, appreciation, and critical observation regarding the video and its content.', additional_kwargs={}, response_metadata={}, name='Topic')], 'documents': [Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified', '_id': '664212477d8c41a4a887161b2662a2ed', '_collection_name': 'video_sentiment_data'}, page_content='**Comment 34:**\\n- Author: @janosorcsik\\n- Likes: 0\\n- Published: 2025-10-15T17:03:26Z\\n- Text: Why is what the guy is saying different from what&#39;s shown in the video?\\n---\\n\\n**Comment 35:**\\n- Author: @repoles\\n- Likes: 0\\n- Published: 2025-10-15T16:57:11Z\\n- Text: My eyes hurt with this color scheme 😎\\n---\\n\\n**Comment 36:**\\n- Author: @xeoxaz\\n- Likes: 1\\n- Published: 2025-10-15T16:45:32Z\\n- Text: Is this whole video AI?\\n---'), Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified', '_id': 'b13ff63d71d3432c982488778f3a612b', '_collection_name': 'video_sentiment_data'}, page_content='**Comment 19:**\\n- Author: @endoflevelboss\\n- Likes: 0\\n- Published: 2025-10-16T08:10:49Z\\n- Text: How did that guy&#39;s haircut be allowed on camera?\\n---\\n\\n**Comment 20:**\\n- Author: @stym06\\n- Likes: 1\\n- Published: 2025-10-16T04:58:35Z\\n- Text: Adam Driver is great in this video\\n---\\n\\n**Comment 21:**\\n- Author: @outsidefive\\n- Likes: 0\\n- Published: 2025-10-16T03:56:19Z\\n- Text: Hey why is it not working? I don&#39;t know, ask GPT. Rip Software Engineering.\\n---\\n\\n**Comment 22:**\\n- Author: @AnonDev77\\n- Likes: 0\\n- Published: 2025-10-16T02:17:56Z\\n- Text: get Eson(?) on more of these videos.  thumbs up. loving codex\\n---'), Document(metadata={'type': 'comment', 'comment_index': 34, 'author': '@janosorcsik', 'likes': 0, 'published': '2025-10-15T17:03:26Z', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', '_id': 'fd56babaed2141bdb7844d6fa065d387', '_collection_name': 'video_sentiment_data'}, page_content='Why is what the guy is saying different from what&#39;s shown in the video?'), Document(metadata={'type': 'comment', 'comment_index': 27, 'author': '@dandragomir6413', 'likes': 0, 'published': '2025-10-16T00:23:30Z', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', '_id': '3b479fbeef424023b61a56c8fd770b9d', '_collection_name': 'video_sentiment_data'}, page_content='are all comments here written by bots? thats what it seems like'), Document(metadata={'type': 'video_context', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', 'channel': 'OpenAI', 'source': 'youtube_unified', '_id': '5e4648f75d1247b2ba9f190b279df0ac', '_collection_name': 'video_sentiment_data'}, page_content='## Video Transcript'), Document(metadata={'type': 'comment', 'comment_index': 36, 'author': '@xeoxaz', 'likes': 1, 'published': '2025-10-15T16:45:32Z', 'video_id': 'iqNzfK4_meQ', 'title': 'Using OpenAI Codex CLI with GPT-5-Codex', '_id': '152d6d7b0eb746698580527d604c420a', '_collection_name': 'video_sentiment_data'}, page_content='Is this whole video AI?')]}}\n",
      "---\n",
      "{'SuperSupervisor': {'next': 'FINISH'}}\n",
      "---\n",
      "{'SuperSupervisor': {'next': 'FINISH'}}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Use the graph\n",
    "\n",
    "for s in compiled_super_graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Tell me the overall sentiment and topics that arise in the comments of this video\"\n",
    "            )\n",
    "        ],\n",
    "        \"documents\": []\n",
    "    },\n",
    "    {\"recursion_limit\": 30},\n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ef4504",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "263395e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from ragas.llms.base import llm_factory\n",
    "from ragas.embeddings import OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "173848b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_llm = llm_factory(\"gpt-4.1-nano\")\n",
    "ragas_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", client='open_ai')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aea2b41",
   "metadata": {},
   "source": [
    "#### Advanced Retrieval Method "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6b1406",
   "metadata": {},
   "source": [
    "We need to construct our multi-agent but this time with the advanced retrieval. Nonetheless the component we are going to evaluate is the retriever."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ce8747",
   "metadata": {},
   "source": [
    "##### Compression retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6d74b057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the adavanced retrieval method we use cohere rerank compression\n",
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank\n",
    "\n",
    "# Initialize Cohere Rerank compressor\n",
    "compressor = CohereRerank(model=\"rerank-v3.5\", top_n=4)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=qdrant_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c3af1c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We repeate some steps for visibility\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    response: str\n",
    "\n",
    "def retrieve_with_reranker(state: State):\n",
    "    retrieved_docs = compression_retriever.invoke(state['question'])\n",
    "    return {\"context\": retrieved_docs}\n",
    "    \n",
    "\n",
    "def generate(state: State):\n",
    "    generator_chain = chat_prompt | generator_llm | StrOutputParser()\n",
    "    response = generator_chain.invoke({\"query\": state['question'], \"context\": state[\"context\"]})\n",
    "    return {'response': response}\n",
    "\n",
    "advanced_rag_graph = StateGraph(State).add_sequence([retrieve_with_reranker, generate])\n",
    "advanced_rag_graph.add_edge(START, \"retrieve_with_reranker\")\n",
    "compiled_advanced_rag_graph = advanced_rag_graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da11a562",
   "metadata": {},
   "source": [
    "#### Multi-query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "48aaab84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=qdrant_retriever, llm= generator_llm\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "507c8ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We repeate some steps for visibility\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    response: str\n",
    "\n",
    "def retrieve_with_multi_query(state: State):\n",
    "    retrieved_docs = multi_query_retriever.invoke(state['question'])\n",
    "    return {\"context\": retrieved_docs}\n",
    "    \n",
    "\n",
    "def generate(state: State):\n",
    "    generator_chain = chat_prompt | generator_llm | StrOutputParser()\n",
    "    response = generator_chain.invoke({\"query\": state['question'], \"context\": state[\"context\"]})\n",
    "    return {'response': response}\n",
    "\n",
    "multi_query_rag_graph = StateGraph(State).add_sequence([retrieve_with_multi_query, generate])\n",
    "multi_query_rag_graph.add_edge(START, \"retrieve_with_multi_query\")\n",
    "compiled_multi_query_rag_graph = multi_query_rag_graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ee1998",
   "metadata": {},
   "source": [
    "#### BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c4ec553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(docs_for_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "58dfa656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We repeate some steps for visibility\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    response: str\n",
    "\n",
    "def bm25_retrieve(state: State):\n",
    "    retrieved_docs = bm25_retriever.invoke(state['question'])\n",
    "    return {\"context\": retrieved_docs}\n",
    "    \n",
    "\n",
    "def generate(state: State):\n",
    "    generator_chain = chat_prompt | generator_llm | StrOutputParser()\n",
    "    response = generator_chain.invoke({\"query\": state['question'], \"context\": state[\"context\"]})\n",
    "    return {'response': response}\n",
    "\n",
    "bm25_rag_graph = StateGraph(State).add_sequence([bm25_retrieve, generate])\n",
    "bm25_rag_graph.add_edge(START, \"bm25_retrieve\")\n",
    "compiled_bm25_rag_graph = bm25_rag_graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79433c78",
   "metadata": {},
   "source": [
    "#### Synthetic Data Generation with RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "88e30ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Inés\\AppData\\Local\\Temp\\ipykernel_274228\\3429563670.py:7: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use the modern LLM providers instead: from ragas.llms.base import llm_factory; llm = llm_factory('gpt-4o-mini') or from ragas.llms.base import instructor_llm_factory; llm = instructor_llm_factory('openai', client=openai_client)\n",
      "  ragas_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-nano\"))\n",
      "C:\\Users\\Inés\\AppData\\Local\\Temp\\ipykernel_274228\\3429563670.py:8: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings\n",
      "  ragas_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
      "C:\\Users\\Inés\\AppData\\Local\\Temp\\ipykernel_274228\\3429563670.py:8: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings\n",
      "  ragas_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n"
     ]
    }
   ],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Create actual LangChain model instances, then wrap them\n",
    "ragas_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-nano\"))\n",
    "ragas_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=\"text-embedding-3-small\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4f3d3fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KnowledgeGraph(nodes: 0, relationships: 0)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Knowledge graph\n",
    "from ragas.testset.graph import KnowledgeGraph\n",
    "\n",
    "kg = KnowledgeGraph()\n",
    "kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5a2485d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KnowledgeGraph(nodes: 88, relationships: 0)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add document to knowledge graph\n",
    "from ragas.testset.graph import Node, NodeType\n",
    "\n",
    "\n",
    "for doc in docs_for_store:\n",
    "    kg.nodes.append(\n",
    "        Node(\n",
    "            type=NodeType.DOCUMENT,\n",
    "            properties={\"page_content\": doc.page_content, \"document_metadata\": doc.metadata}\n",
    "        )\n",
    "    )\n",
    "kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0d5e2a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying SummaryExtractor:   0%|          | 0/36 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying SummaryExtractor: 100%|██████████| 36/36 [00:46<00:00,  1.28s/it]\n",
      "Applying CustomNodeFilter:   0%|          | 0/88 [00:00<?, ?it/s]Node 79296a88-5737-4d4b-99eb-9fa15820173a does not have a summary. Skipping filtering.\n",
      "Node 8c447ccb-b4f5-4ccd-bc9e-d13b09de4c9b does not have a summary. Skipping filtering.\n",
      "Node bd2a18ef-fbff-467d-b8a9-e69ac4dcc3b5 does not have a summary. Skipping filtering.\n",
      "Applying SummaryExtractor: 100%|██████████| 36/36 [00:46<00:00,  1.28s/it]\n",
      "Applying CustomNodeFilter:   0%|          | 0/88 [00:00<?, ?it/s]Node 79296a88-5737-4d4b-99eb-9fa15820173a does not have a summary. Skipping filtering.\n",
      "Node 8c447ccb-b4f5-4ccd-bc9e-d13b09de4c9b does not have a summary. Skipping filtering.\n",
      "Node bd2a18ef-fbff-467d-b8a9-e69ac4dcc3b5 does not have a summary. Skipping filtering.\n",
      "Node a9fb7410-d5b4-4e85-9167-d930e695d0b1 does not have a summary. Skipping filtering.\n",
      "Node 80a20a0e-531b-4a41-95f2-65b787e99e22 does not have a summary. Skipping filtering.\n",
      "Node 0de0959e-5892-49b3-ae0f-b6d707d13756 does not have a summary. Skipping filtering.\n",
      "Node a9fb7410-d5b4-4e85-9167-d930e695d0b1 does not have a summary. Skipping filtering.\n",
      "Node 80a20a0e-531b-4a41-95f2-65b787e99e22 does not have a summary. Skipping filtering.\n",
      "Node 0de0959e-5892-49b3-ae0f-b6d707d13756 does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:   8%|▊         | 7/88 [00:02<00:24,  3.24it/s]Node 18be3d9b-72e8-4c4c-97b4-83686bdd55f4 does not have a summary. Skipping filtering.\n",
      "Node fd282bd6-2fb9-48b2-85ba-d3a5cc625b8b does not have a summary. Skipping filtering.\n",
      "Node f9234bab-803b-49be-82e5-b49704ef4b0e does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:   9%|▉         | 8/88 [00:21<04:42,  3.53s/it]Node 18be3d9b-72e8-4c4c-97b4-83686bdd55f4 does not have a summary. Skipping filtering.\n",
      "Node fd282bd6-2fb9-48b2-85ba-d3a5cc625b8b does not have a summary. Skipping filtering.\n",
      "Node f9234bab-803b-49be-82e5-b49704ef4b0e does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:   9%|▉         | 8/88 [00:21<04:42,  3.53s/it]Node 685579e7-855e-4e81-95bd-34270a94ec9a does not have a summary. Skipping filtering.\n",
      "Node 8590807c-7430-4409-b368-82d8f8108669 does not have a summary. Skipping filtering.\n",
      "Node 685579e7-855e-4e81-95bd-34270a94ec9a does not have a summary. Skipping filtering.\n",
      "Node 8590807c-7430-4409-b368-82d8f8108669 does not have a summary. Skipping filtering.\n",
      "Node b351d85a-f200-4b84-80c5-176f72de95f1 does not have a summary. Skipping filtering.\n",
      "Node e5a7c558-f157-4448-b882-6943ec0bd6de does not have a summary. Skipping filtering.\n",
      "Node e11454b9-473b-463c-9476-e2701f6859b9 does not have a summary. Skipping filtering.\n",
      "Node 9240bf13-c902-4c89-98db-d1cb13d68950 does not have a summary. Skipping filtering.\n",
      "Node 3a1ea256-abcb-4357-9352-81a05f6b7fb1 does not have a summary. Skipping filtering.\n",
      "Node f6afb7ee-c59b-4921-a357-de174dedb8da does not have a summary. Skipping filtering.\n",
      "Node b351d85a-f200-4b84-80c5-176f72de95f1 does not have a summary. Skipping filtering.\n",
      "Node e5a7c558-f157-4448-b882-6943ec0bd6de does not have a summary. Skipping filtering.\n",
      "Node e11454b9-473b-463c-9476-e2701f6859b9 does not have a summary. Skipping filtering.\n",
      "Node 9240bf13-c902-4c89-98db-d1cb13d68950 does not have a summary. Skipping filtering.\n",
      "Node 3a1ea256-abcb-4357-9352-81a05f6b7fb1 does not have a summary. Skipping filtering.\n",
      "Node f6afb7ee-c59b-4921-a357-de174dedb8da does not have a summary. Skipping filtering.\n",
      "Node ccf22163-bf8e-4a43-ad94-638c3f6cdcb2 does not have a summary. Skipping filtering.\n",
      "Node ccf22163-bf8e-4a43-ad94-638c3f6cdcb2 does not have a summary. Skipping filtering.\n",
      "Node f5d55b96-09cd-4df7-bef5-f2b65c32ff6c does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  40%|███▉      | 35/88 [00:22<00:26,  1.97it/s]Node f5d55b96-09cd-4df7-bef5-f2b65c32ff6c does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  40%|███▉      | 35/88 [00:22<00:26,  1.97it/s]Node 01b578a4-a9f8-44df-ab05-70900848fd89 does not have a summary. Skipping filtering.\n",
      "Node 09cb6369-4edf-42aa-8e87-6b9a8ecfd359 does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  42%|████▏     | 37/88 [00:23<00:26,  1.92it/s]Node f940ae63-ffb8-4fde-b5e7-55ac0aa8733b does not have a summary. Skipping filtering.\n",
      "Node 01b578a4-a9f8-44df-ab05-70900848fd89 does not have a summary. Skipping filtering.\n",
      "Node 09cb6369-4edf-42aa-8e87-6b9a8ecfd359 does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  42%|████▏     | 37/88 [00:23<00:26,  1.92it/s]Node f940ae63-ffb8-4fde-b5e7-55ac0aa8733b does not have a summary. Skipping filtering.\n",
      "Node fa478e22-3eb8-4c93-b600-5c9fb2c84432 does not have a summary. Skipping filtering.\n",
      "Node 4ab736ba-9256-419b-9200-3b4f917ac41b does not have a summary. Skipping filtering.\n",
      "Node 2bf30440-2848-4043-9a35-6d74d635c377 does not have a summary. Skipping filtering.\n",
      "Node 1feec062-8e6f-4db1-83b1-538bf7f10fac does not have a summary. Skipping filtering.\n",
      "Node 1dff4706-85ad-432f-9325-14ea05df9fe8 does not have a summary. Skipping filtering.\n",
      "Node eb71a0c4-3360-449f-b36b-782b6f586a6e does not have a summary. Skipping filtering.\n",
      "Node 1278d05f-29b4-43d6-bd92-b40fbd4d3980 does not have a summary. Skipping filtering.\n",
      "Node d0c7fc1a-4585-4cfa-86c6-253ba7d12466 does not have a summary. Skipping filtering.\n",
      "Node b1babbc6-0530-4d81-93a8-83addc70d857 does not have a summary. Skipping filtering.\n",
      "Node fa478e22-3eb8-4c93-b600-5c9fb2c84432 does not have a summary. Skipping filtering.\n",
      "Node 4ab736ba-9256-419b-9200-3b4f917ac41b does not have a summary. Skipping filtering.\n",
      "Node 2bf30440-2848-4043-9a35-6d74d635c377 does not have a summary. Skipping filtering.\n",
      "Node 1feec062-8e6f-4db1-83b1-538bf7f10fac does not have a summary. Skipping filtering.\n",
      "Node 1dff4706-85ad-432f-9325-14ea05df9fe8 does not have a summary. Skipping filtering.\n",
      "Node eb71a0c4-3360-449f-b36b-782b6f586a6e does not have a summary. Skipping filtering.\n",
      "Node 1278d05f-29b4-43d6-bd92-b40fbd4d3980 does not have a summary. Skipping filtering.\n",
      "Node d0c7fc1a-4585-4cfa-86c6-253ba7d12466 does not have a summary. Skipping filtering.\n",
      "Node b1babbc6-0530-4d81-93a8-83addc70d857 does not have a summary. Skipping filtering.\n",
      "Node fefacd4d-4bf6-4ddb-ab38-df81644e7810 does not have a summary. Skipping filtering.\n",
      "Node fefacd4d-4bf6-4ddb-ab38-df81644e7810 does not have a summary. Skipping filtering.\n",
      "Node 5990d999-a65f-4eb0-be37-387b65cb8ea1 does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  56%|█████▌    | 49/88 [00:28<00:18,  2.14it/s]Node 63fcbdf6-0fc9-412d-a604-93949ca135b7 does not have a summary. Skipping filtering.\n",
      "Node 2429b255-1563-4891-9243-4f32323b2f1d does not have a summary. Skipping filtering.\n",
      "Node 809253aa-6e4b-4ce8-a5be-63f9e9e9ee47 does not have a summary. Skipping filtering.\n",
      "Node e69c27ec-9517-4aa3-9cf5-422ac64c17e3 does not have a summary. Skipping filtering.\n",
      "Node 296055bb-e34b-499b-8e5b-83c9181fa401 does not have a summary. Skipping filtering.\n",
      "Node 0d10556c-9cb8-467e-9493-eb4806829052 does not have a summary. Skipping filtering.\n",
      "Node 6c504f63-23c8-4c7f-b64c-bb992b1f4cc7 does not have a summary. Skipping filtering.\n",
      "Node 2d35f821-a075-4bc4-b368-f9d69f0a5079 does not have a summary. Skipping filtering.\n",
      "Node 5990d999-a65f-4eb0-be37-387b65cb8ea1 does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  56%|█████▌    | 49/88 [00:28<00:18,  2.14it/s]Node 63fcbdf6-0fc9-412d-a604-93949ca135b7 does not have a summary. Skipping filtering.\n",
      "Node 2429b255-1563-4891-9243-4f32323b2f1d does not have a summary. Skipping filtering.\n",
      "Node 809253aa-6e4b-4ce8-a5be-63f9e9e9ee47 does not have a summary. Skipping filtering.\n",
      "Node e69c27ec-9517-4aa3-9cf5-422ac64c17e3 does not have a summary. Skipping filtering.\n",
      "Node 296055bb-e34b-499b-8e5b-83c9181fa401 does not have a summary. Skipping filtering.\n",
      "Node 0d10556c-9cb8-467e-9493-eb4806829052 does not have a summary. Skipping filtering.\n",
      "Node 6c504f63-23c8-4c7f-b64c-bb992b1f4cc7 does not have a summary. Skipping filtering.\n",
      "Node 2d35f821-a075-4bc4-b368-f9d69f0a5079 does not have a summary. Skipping filtering.\n",
      "Node 1dde3710-090b-4640-8840-06b629f7fcd7 does not have a summary. Skipping filtering.\n",
      "Node 1dde3710-090b-4640-8840-06b629f7fcd7 does not have a summary. Skipping filtering.\n",
      "Node 5b2e5c67-faea-4a11-9167-e28c743d6087 does not have a summary. Skipping filtering.\n",
      "Node aa48d9da-b14e-4677-ae02-3e9e564cc7b6 does not have a summary. Skipping filtering.\n",
      "Node e7d7dc38-6949-4ccf-9129-cf0586f190c7 does not have a summary. Skipping filtering.\n",
      "Node ac4d0952-6d63-4349-a2ba-45b0c3030ade does not have a summary. Skipping filtering.\n",
      "Node f6d30b3d-8ba4-4abf-a8d0-ddef391cd875 does not have a summary. Skipping filtering.\n",
      "Node 5b2e5c67-faea-4a11-9167-e28c743d6087 does not have a summary. Skipping filtering.\n",
      "Node aa48d9da-b14e-4677-ae02-3e9e564cc7b6 does not have a summary. Skipping filtering.\n",
      "Node e7d7dc38-6949-4ccf-9129-cf0586f190c7 does not have a summary. Skipping filtering.\n",
      "Node ac4d0952-6d63-4349-a2ba-45b0c3030ade does not have a summary. Skipping filtering.\n",
      "Node f6d30b3d-8ba4-4abf-a8d0-ddef391cd875 does not have a summary. Skipping filtering.\n",
      "Node 070bf5c7-72ea-47e3-9d32-f2b3c77268d6 does not have a summary. Skipping filtering.\n",
      "Node 7b77e06d-cff0-417f-9a9e-dfcdb0535371 does not have a summary. Skipping filtering.\n",
      "Node bd9c6ed4-de06-49bf-af64-63170d4a6f1a does not have a summary. Skipping filtering.\n",
      "Node a42160e7-010d-4d06-80db-045c910f660f does not have a summary. Skipping filtering.\n",
      "Node a5161e37-ca33-4280-94ae-bafd111ff8c5 does not have a summary. Skipping filtering.\n",
      "Node 070bf5c7-72ea-47e3-9d32-f2b3c77268d6 does not have a summary. Skipping filtering.\n",
      "Node 7b77e06d-cff0-417f-9a9e-dfcdb0535371 does not have a summary. Skipping filtering.\n",
      "Node bd9c6ed4-de06-49bf-af64-63170d4a6f1a does not have a summary. Skipping filtering.\n",
      "Node a42160e7-010d-4d06-80db-045c910f660f does not have a summary. Skipping filtering.\n",
      "Node a5161e37-ca33-4280-94ae-bafd111ff8c5 does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter: 100%|██████████| 88/88 [00:45<00:00,  1.94it/s]\n",
      "Applying CustomNodeFilter: 100%|██████████| 88/88 [00:45<00:00,  1.94it/s]\n",
      "Applying EmbeddingExtractor: 100%|██████████| 33/33 [00:20<00:00,  1.64it/s]\n",
      "Applying EmbeddingExtractor: 100%|██████████| 33/33 [00:20<00:00,  1.64it/s]\n",
      "Applying ThemesExtractor: 100%|██████████| 85/85 [02:00<00:00,  1.42s/it]\n",
      "Applying ThemesExtractor: 100%|██████████| 85/85 [02:00<00:00,  1.42s/it]\n",
      "Applying NERExtractor: 100%|██████████| 85/85 [01:42<00:00,  1.20s/it]\n",
      "Applying NERExtractor: 100%|██████████| 85/85 [01:42<00:00,  1.20s/it]]\n",
      "Applying CosineSimilarityBuilder: 100%|██████████| 1/1 [00:00<00:00, 98.37it/s]\n",
      "Applying CosineSimilarityBuilder: 100%|██████████| 1/1 [00:00<00:00, 98.37it/s]\n",
      "Applying OverlapScoreBuilder: 100%|██████████| 1/1 [00:00<00:00, 152.22it/s]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KnowledgeGraph(nodes: 85, relationships: 182)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying the transformations\n",
    "from ragas.testset.transforms import default_transforms, apply_transforms\n",
    "\n",
    "transformer_llm = ragas_llm\n",
    "embedding_model = ragas_embeddings\n",
    "\n",
    "default_transforms = default_transforms(documents=docs_for_store, llm=transformer_llm, embedding_model=embedding_model)\n",
    "apply_transforms(kg, default_transforms)\n",
    "kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e35a3e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KnowledgeGraph(nodes: 85, relationships: 182)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the knowledge graph\n",
    "\n",
    "kg.save(\"comments_kg.json\")\n",
    "comments_kg = KnowledgeGraph.load(\"comments_kg.json\")\n",
    "comments_kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "08418767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Personas\n",
    "\n",
    "from ragas.testset.persona import Persona\n",
    "\n",
    "persona_content_creator = Persona(\n",
    "    name=\"Content Creator\",\n",
    "    role_description=(\n",
    "        \"A YouTuber or content creator seeking to understand audience reception. Wants quick insights \"\n",
    "        \"into what resonates with viewers: Are comments positive or negative? What topics do viewers \"\n",
    "        \"care about most? Needs actionable feedback to improve future content. Prefers summarized \"\n",
    "        \"sentiment patterns, top discussion themes, and specific comment examples that highlight \"\n",
    "        \"viewer concerns or praise.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "persona_brand_manager = Persona(\n",
    "    name=\"Brand Manager\",\n",
    "    role_description=(\n",
    "        \"A marketing professional monitoring brand reputation and campaign performance. Needs detailed \"\n",
    "        \"sentiment breakdowns with metrics: What percentage is positive/negative/neutral? Are there \"\n",
    "        \"any concerning negative trends or controversy signals? Wants to identify brand mentions, \"\n",
    "        \"product feedback, and potential PR issues. Requires data-backed insights with comment counts \"\n",
    "        \"and sentiment distributions for stakeholder reports.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "persona_researcher = Persona(\n",
    "    name=\"Academic Researcher\",\n",
    "    role_description=(\n",
    "        \"A researcher studying online discourse and audience behavior. Seeks comprehensive thematic \"\n",
    "        \"analysis: What underlying topics emerge? How do sentiments correlate with video content? \"\n",
    "        \"Wants nuanced categorization beyond simple positive/negative labels. Interested in identifying \"\n",
    "        \"patterns, anomalies, and contextual factors that shape viewer reactions. Prefers detailed \"\n",
    "        \"topic clusters with supporting evidence from multiple comments.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "personas = [persona_content_creator, persona_brand_manager, persona_researcher]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "70c71b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generator\n",
    "\n",
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "generator = TestsetGenerator(llm=ragas_llm, embedding_model=ragas_embeddings, knowledge_graph=comments_kg, persona_list=personas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3a1e0cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Synthesizers\n",
    "\n",
    "from ragas.testset.synthesizers import default_query_distribution, SingleHopSpecificQuerySynthesizer\n",
    "\n",
    "query_distribution = [\n",
    "        (SingleHopSpecificQuerySynthesizer(llm=generator_llm), 1),\n",
    "      \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d391ac4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Scenarios: 100%|██████████| 1/1 [00:22<00:00, 22.10s/it]\n",
      "Generating Scenarios: 100%|██████████| 1/1 [00:22<00:00, 22.10s/it]\n",
      "Generating Samples: 100%|██████████| 10/10 [00:07<00:00,  1.41it/s]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is GPT-5-Codex and how is it used in the ...</td>\n",
       "      <td>[# VIDEO ANALYSIS CONTEXT\\n\\n## Video Informat...</td>\n",
       "      <td>The video titled \"Using OpenAI Codex CLI with ...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Based on your research focus on online discour...</td>\n",
       "      <td>[To learn more and get started:\\n\\nCodex: open...</td>\n",
       "      <td>The provided context indicates that openai.com...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the significance of the code name 'Cod...</td>\n",
       "      <td>[Hey, what are you working on? &gt;&gt; I'm trying t...</td>\n",
       "      <td>In the provided context, 'Codeex CL164' is men...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the emerging topics related to GPT5 i...</td>\n",
       "      <td>[easily with either mpm or brew and log in wit...</td>\n",
       "      <td>The context does not provide specific informat...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Codex do what?</td>\n",
       "      <td>[multiplayer feature, why don't you kind of op...</td>\n",
       "      <td>The context mentions Codex in relation to mode...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is Codex?</td>\n",
       "      <td>[get into the sandboxing features of codecs wh...</td>\n",
       "      <td>Codex has sandboxing features with three modes...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>So, uh, like, what do viewers think about Code...</td>\n",
       "      <td>[by trying to edit things. And then we have Co...</td>\n",
       "      <td>The context provides information about Codex b...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Why should we deploy the app on Versal and how...</td>\n",
       "      <td>[Why is this showing up? Go look at the logs. ...</td>\n",
       "      <td>In the context, the suggestion is to deploy th...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How is the term \"Versel\" used in the context o...</td>\n",
       "      <td>[you need persistence or maybe you want to loo...</td>\n",
       "      <td>In the provided context, \"Versel\" is mentioned...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Based on the context, what are the key functio...</td>\n",
       "      <td>[super in sync. &gt;&gt; Incredible. &gt;&gt; This is all ...</td>\n",
       "      <td>The context highlights that Codex CLI is capab...</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  What is GPT-5-Codex and how is it used in the ...   \n",
       "1  Based on your research focus on online discour...   \n",
       "2  What is the significance of the code name 'Cod...   \n",
       "3  What are the emerging topics related to GPT5 i...   \n",
       "4                                     Codex do what?   \n",
       "5                                     What is Codex?   \n",
       "6  So, uh, like, what do viewers think about Code...   \n",
       "7  Why should we deploy the app on Versal and how...   \n",
       "8  How is the term \"Versel\" used in the context o...   \n",
       "9  Based on the context, what are the key functio...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [# VIDEO ANALYSIS CONTEXT\\n\\n## Video Informat...   \n",
       "1  [To learn more and get started:\\n\\nCodex: open...   \n",
       "2  [Hey, what are you working on? >> I'm trying t...   \n",
       "3  [easily with either mpm or brew and log in wit...   \n",
       "4  [multiplayer feature, why don't you kind of op...   \n",
       "5  [get into the sandboxing features of codecs wh...   \n",
       "6  [by trying to edit things. And then we have Co...   \n",
       "7  [Why is this showing up? Go look at the logs. ...   \n",
       "8  [you need persistence or maybe you want to loo...   \n",
       "9  [super in sync. >> Incredible. >> This is all ...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  The video titled \"Using OpenAI Codex CLI with ...   \n",
       "1  The provided context indicates that openai.com...   \n",
       "2  In the provided context, 'Codeex CL164' is men...   \n",
       "3  The context does not provide specific informat...   \n",
       "4  The context mentions Codex in relation to mode...   \n",
       "5  Codex has sandboxing features with three modes...   \n",
       "6  The context provides information about Codex b...   \n",
       "7  In the context, the suggestion is to deploy th...   \n",
       "8  In the provided context, \"Versel\" is mentioned...   \n",
       "9  The context highlights that Codex CLI is capab...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "0  single_hop_specific_query_synthesizer  \n",
       "1  single_hop_specific_query_synthesizer  \n",
       "2  single_hop_specific_query_synthesizer  \n",
       "3  single_hop_specific_query_synthesizer  \n",
       "4  single_hop_specific_query_synthesizer  \n",
       "5  single_hop_specific_query_synthesizer  \n",
       "6  single_hop_specific_query_synthesizer  \n",
       "7  single_hop_specific_query_synthesizer  \n",
       "8  single_hop_specific_query_synthesizer  \n",
       "9  single_hop_specific_query_synthesizer  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Generate testset\n",
    "\n",
    "testset = generator.generate(testset_size=10, query_distribution=query_distribution)\n",
    "testset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f247d2d",
   "metadata": {},
   "source": [
    "#### Start the evaluation!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a9021e",
   "metadata": {},
   "source": [
    "The metrics:\n",
    "\n",
    "- faithfulness\n",
    "- response relevance\n",
    "- context precision\n",
    "- context recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d5218a",
   "metadata": {},
   "source": [
    "#### Naive Retrival Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "4044c79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the queries we generated with SDG to get context and responses\n",
    "import copy\n",
    "naive_testset = copy.deepcopy(testset)\n",
    "\n",
    "for test_row in naive_testset:\n",
    "    response = compiled_rag_graph.invoke({\"question\": test_row.eval_sample.user_input})\n",
    "    test_row.eval_sample.response = response[\"response\"]\n",
    "    test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "98edc50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided comments, Codex appears to be a tool or AI system that helps developers create software. One commenter mentions that \"Codex is my best friend lately,\" and another describes GPT-5-Codex as \"the most fun a developer can have creating software,\" indicating that it is used for programming or software development tasks. Additionally, a comment raises concerns about giving Codex \"full access,\" suggesting that it has capabilities that can interact with APIs like Vercel, and users are interested in controlling what it is allowed to do. Overall, in the context of the video, Codex is an AI assistant or system integrated with CLI tools that assists in building and deploying software applications.'"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_testset.samples[0].eval_sample.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "23bbccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Testset into Evaluation schema\n",
    "from ragas import EvaluationDataset\n",
    "evaluation_dataset = EvaluationDataset.from_pandas(naive_testset.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "b4ced3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judge model for evaluation\n",
    "from ragas.llms.base import llm_factory\n",
    "\n",
    "evaluator_llm = llm_factory('gpt-4o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8ff411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef01cbe2ee7b4cb696c07effb15730ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'faithfulness': 0.9079, 'context_recall': 0.9000, 'context_precision': 0.6333, 'answer_relevancy': 0.8968}"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation on desired metrics\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_relevancy\n",
    ")\n",
    "from ragas import evaluate, RunConfig\n",
    "\n",
    "custom_run_config = RunConfig(timeout=360)\n",
    "\n",
    "baseline_result = evaluate(\n",
    "    dataset = evaluation_dataset,\n",
    "    metrics = [\n",
    "         faithfulness,\n",
    "         context_recall,\n",
    "         context_precision,\n",
    "         answer_relevancy\n",
    "    ],\n",
    "    llm = evaluator_llm,\n",
    "    run_config = custom_run_config\n",
    ")\n",
    "\n",
    "baseline_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931a3dc5",
   "metadata": {},
   "source": [
    "#### Compression Reranker Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "c98bae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run queries through advanced RaG graph \n",
    "import time\n",
    "\n",
    "advanced_testset = copy.deepcopy(testset)\n",
    "\n",
    "for test_row in advanced_testset:\n",
    "    response = compiled_advanced_rag_graph.invoke({\"question\": test_row.eval_sample.user_input})\n",
    "    test_row.eval_sample.response = response[\"response\"]\n",
    "    test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "62093bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided comments, the video discusses Codex as a tool capable of generating software, with one user describing GPT-5-Codex as \"the most fun a developer can have creating software\" and likening it to conjuring new apps and features. Another comment highlights concerns about its capabilities, implying that Codex can perform complex coding tasks, potentially with a degree of autonomy. A comment also humorously notes that code generated by Codex may have security holes, suggesting it produces imperfect code. Overall, in this context, Codex is presented as an AI-powered coding assistant or system that can help automate or generate code, making software development more accessible and creative, though with some concerns about control and security.'"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check\n",
    "advanced_testset.samples[0].eval_sample.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "23eba22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Testset into Evaluation schema\n",
    "compressor_evaluation_dataset = EvaluationDataset.from_pandas(advanced_testset.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab5a129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20402eaa5fad4553a8cfc739936edb32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "\n",
    "advanced_result = evaluate(\n",
    "    dataset = compressor_evaluation_dataset,\n",
    "    metrics= [\n",
    "         faithfulness,\n",
    "         context_recall,\n",
    "         context_precision,\n",
    "         answer_relevancy\n",
    "    ],\n",
    "    llm = evaluator_llm,\n",
    "    run_config = custom_run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "7d6a6b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'faithfulness': 0.8946, 'context_recall': 0.8500, 'context_precision': 0.8750, 'answer_relevancy': 0.8870}"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advanced_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332b72d4",
   "metadata": {},
   "source": [
    "#### Multi-Query evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "48c791d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run queries through advanced RaG graph \n",
    "import time\n",
    "\n",
    "multi_query_testset = copy.deepcopy(testset)\n",
    "\n",
    "for test_row in multi_query_testset:\n",
    "    response = compiled_multi_query_rag_graph.invoke({\"question\": test_row.eval_sample.user_input})\n",
    "    test_row.eval_sample.response = response[\"response\"]\n",
    "    test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "e0f16c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided comments and context, Codex appears to be a powerful AI tool related to software development that allows users to create applications and perform coding tasks. Comments mention that Codex is like a friendly assistant or \"best friend\" for developers, helping them generate code, understand projects better, and even argue against incorrect prompts, indicating its usefulness in coding and project understanding. Some comments also express concerns about permissions and control when using Codex, suggesting it can perform operations like accessing APIs. Overall, Codex in this context is an AI system used to assist in programming, code generation, and project development.'"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check\n",
    "multi_query_testset.samples[0].eval_sample.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "a3829bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_query_evaluation_dataset = EvaluationDataset.from_pandas(multi_query_testset.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "e067e3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e3cf46ae8c4774ae057efe58c4fdae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n"
     ]
    }
   ],
   "source": [
    "multi_query_result = evaluate(\n",
    "    dataset = multi_query_evaluation_dataset,\n",
    "    metrics= [\n",
    "         faithfulness,\n",
    "         context_recall,\n",
    "         context_precision,\n",
    "         answer_relevancy\n",
    "    ],\n",
    "    llm = evaluator_llm,\n",
    "    run_config = custom_run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "b9173111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'faithfulness': 0.8460, 'context_recall': 0.8750, 'context_precision': 0.7097, 'answer_relevancy': 0.8945}"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_query_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f415d897",
   "metadata": {},
   "source": [
    "#### BM25 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "69c06d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run queries through advanced RaG graph \n",
    "import time\n",
    "\n",
    "bm25_testset = copy.deepcopy(testset)\n",
    "\n",
    "for test_row in bm25_testset:\n",
    "    response = compiled_bm25_rag_graph.invoke({\"question\": test_row.eval_sample.user_input})\n",
    "    test_row.eval_sample.response = response[\"response\"]\n",
    "    test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "aeabe8a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, Codex refers to a tool or technology associated with OpenAI that is used in conjunction with GPT-5-Codex. The video demonstrates how to use the Codex CLI (Command Line Interface) to build a multiplayer game without writing code directly. The comments suggest that Codex is involved in AI-related coding or development tasks, possibly enabling automation or simplified programming workflows.'"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_testset.samples[0].eval_sample.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "81b03d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_evaluation_dataset = EvaluationDataset.from_pandas(multi_query_testset.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "b668759d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "858ce5690c95473d99febd89417ba570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n"
     ]
    }
   ],
   "source": [
    "bm25_result = evaluate(\n",
    "    dataset = bm25_evaluation_dataset,\n",
    "    metrics= [\n",
    "        faithfulness,\n",
    "        context_recall,\n",
    "        context_precision,\n",
    "        answer_relevancy\n",
    "    ],\n",
    "    llm = evaluator_llm,\n",
    "    run_config = custom_run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "a6a16612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'faithfulness': 0.9186, 'context_recall': 0.9000, 'context_precision': 0.8667, 'answer_relevancy': 0.8975}"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796b2195",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "a6af16f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison tables (absolute values and baseline deltas)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Collect evaluation frames\n",
    "baseline_df = baseline_result.to_pandas()\n",
    "advanced_df = advanced_result.to_pandas()\n",
    "multi_query_df = multi_query_result.to_pandas()\n",
    "bm25_df = bm25_result.to_pandas()\n",
    "\n",
    "exclude_columns = ['user_input', 'retrieved_contexts', 'response', 'reference']\n",
    "metric_names = [col for col in baseline_df.columns if col not in exclude_columns]\n",
    "\n",
    "absolute_rows = []\n",
    "relative_rows = []\n",
    "\n",
    "def pct_change(base, new):\n",
    "    if base is None or base == 0:\n",
    "        return np.nan\n",
    "    return round(((new - base) / base) * 100, 2)\n",
    "\n",
    "for metric in metric_names:\n",
    "    series_list = [df[metric] for df in (baseline_df, advanced_df, multi_query_df, bm25_df)]\n",
    "    if any(series.dtype not in ['float64', 'int64'] for series in series_list):\n",
    "        continue\n",
    "\n",
    "    baseline_val, advanced_val, multi_query_val, bm25_val = [series.mean() for series in series_list]\n",
    "\n",
    "    absolute_rows.append({\n",
    "        'Metric': metric,\n",
    "        'Baseline (Naive)': round(baseline_val, 4),\n",
    "        'Advanced (Cohere Rerank)': round(advanced_val, 4),\n",
    "        'Multi-Query': round(multi_query_val, 4),\n",
    "        'BM25': round(bm25_val, 4)\n",
    "    })\n",
    "\n",
    "    relative_rows.append({\n",
    "        'Metric': metric,\n",
    "        'Advanced Δ': round(advanced_val - baseline_val, 4),\n",
    "        'Advanced %Δ': pct_change(baseline_val, advanced_val),\n",
    "        'Multi-Query Δ': round(multi_query_val - baseline_val, 4),\n",
    "        'Multi-Query %Δ': pct_change(baseline_val, multi_query_val),\n",
    "        'BM25 Δ': round(bm25_val - baseline_val, 4),\n",
    "        'BM25 %Δ': pct_change(baseline_val, bm25_val)\n",
    "    })\n",
    "\n",
    "absolute_df = pd.DataFrame(absolute_rows)\n",
    "relative_df = pd.DataFrame(relative_rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "fc02c80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Baseline (Naive)</th>\n",
       "      <th>Advanced (Cohere Rerank)</th>\n",
       "      <th>Multi-Query</th>\n",
       "      <th>BM25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faithfulness</td>\n",
       "      <td>0.9079</td>\n",
       "      <td>0.8946</td>\n",
       "      <td>0.8460</td>\n",
       "      <td>0.9186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>context_recall</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.8500</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.9000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>context_precision</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.7097</td>\n",
       "      <td>0.8667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>answer_relevancy</td>\n",
       "      <td>0.8968</td>\n",
       "      <td>0.8870</td>\n",
       "      <td>0.8945</td>\n",
       "      <td>0.8975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Metric  Baseline (Naive)  Advanced (Cohere Rerank)  Multi-Query  \\\n",
       "0       faithfulness            0.9079                    0.8946       0.8460   \n",
       "1     context_recall            0.9000                    0.8500       0.8750   \n",
       "2  context_precision            0.6333                    0.8750       0.7097   \n",
       "3   answer_relevancy            0.8968                    0.8870       0.8945   \n",
       "\n",
       "     BM25  \n",
       "0  0.9186  \n",
       "1  0.9000  \n",
       "2  0.8667  \n",
       "3  0.8975  "
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "absolute_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "618c617f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Advanced Δ</th>\n",
       "      <th>Advanced %Δ</th>\n",
       "      <th>Multi-Query Δ</th>\n",
       "      <th>Multi-Query %Δ</th>\n",
       "      <th>BM25 Δ</th>\n",
       "      <th>BM25 %Δ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faithfulness</td>\n",
       "      <td>-0.0132</td>\n",
       "      <td>-1.46</td>\n",
       "      <td>-0.0619</td>\n",
       "      <td>-6.82</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>context_recall</td>\n",
       "      <td>-0.0500</td>\n",
       "      <td>-5.56</td>\n",
       "      <td>-0.0250</td>\n",
       "      <td>-2.78</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>context_precision</td>\n",
       "      <td>0.2417</td>\n",
       "      <td>38.16</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>12.06</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>36.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>answer_relevancy</td>\n",
       "      <td>-0.0098</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>-0.0023</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Metric  Advanced Δ  Advanced %Δ  Multi-Query Δ  Multi-Query %Δ  \\\n",
       "0       faithfulness     -0.0132        -1.46        -0.0619           -6.82   \n",
       "1     context_recall     -0.0500        -5.56        -0.0250           -2.78   \n",
       "2  context_precision      0.2417        38.16         0.0763           12.06   \n",
       "3   answer_relevancy     -0.0098        -1.10        -0.0023           -0.26   \n",
       "\n",
       "   BM25 Δ  BM25 %Δ  \n",
       "0  0.0107     1.18  \n",
       "1  0.0000     0.00  \n",
       "2  0.2333    36.84  \n",
       "3  0.0007     0.08  "
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_df #with respect to naive baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdb23ec",
   "metadata": {},
   "source": [
    ">Note: You can find a more detailed analysis in REPORT.md\n",
    "\n",
    "The problem that can be identified in the baseline is the low **context_precision** (around 63.3%). That means, from the documents retrieved, 37% were irrelevant. \n",
    "\n",
    "All the retrievers improve *context_precision* as seen in the relative gains. Nonetheless, *the ONLY* retriever that increased *context_precision* without sacrificing *context_recall* (not missing information) was BM25. This experiment I repeated 4 times. And the results have been consistent: BM25 improves *IN ALL THE METRICS* with respect to baseline. That is why it has been selected for the prototype.\n",
    "\n",
    "However, for this prototype, a small number of comments were use to build the dataset. Which means that for the final product (when I retrieved all comments) further evaluation needs to be done, to assess if this retriever excells in this particular dataset.\n",
    "\n",
    "This retriever also allows for reducing costs in production as embeddings are not required, which, if chosen is an additional benefit. The nature of the dataset, short comments with little signal for a retriever that makes use of embeddings (hence relies on semantics) could explain why Best Match 25 (sparse retriever that relies on exact match) excels on this task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "youtube-sentiment-backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
